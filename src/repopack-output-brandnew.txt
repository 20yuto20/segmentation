================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-08-07T09:54:15.141Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
conf/
  test_memo.yaml
  test.yaml
  voc_abci.yaml
  voc.yaml
debug/
  check_label.py
  check.py
  find_label.py
load_dataset/
  city.py
  psp_data_augmentation.py
  voc.py
loss/
  entropy.py
make_dataset/
  download/
    conc.py
    download_sbd.py
    download_voc2007.sh
    download_voc2012.sh
    merge.py
    README.md
  check.py
  convert.py
  create_sbd.py
  list.py
model/
  pspnet.py
  resnet.py
  segnet.py
scripts/
  abci/
    run.bash
    test.sh
  cotton/
    run_cotton.sh
    run_park.sh
    run_test.sh
    voc_park.sh
utils/
  common.py
  lr_scheduler.py
  suggest.py
aug_meta.py
augment.py
dataloader.py
evalator.py
main.py
notify.py
ra_temp.py
ra.py
set_cfg.py
test.py
train_val.py

================================================================
Repository Files
================================================================

================
File: aug_meta.py
================
from typing import List, Dict, Optional, Union, Tuple
import math
import matplotlib.pyplot as plt
import numpy as np
import os

from PIL import ImageOps, Image
import torch
import torchvision.transforms.functional as F
from torch import Tensor, nn

from augment import Cutout, solarize_add



# This function will apply to the RandAugment
# TODO: apply to the RA
def _apply_op(
    sample: Dict[str, Tensor],
    op_name: str,
    magnitude: float,
    interpolation: F.InterpolationMode,
    fill: Optional[List[float]]
):
    img, label = sample['image'], sample['label']

    if op_name == "ShearX":
        img = F.affine(
            img,
            angle=0.0,
            translate=[0, 0],
            scale=1.0,
            shear=[math.degrees(math.atan(magnitude)), 0.0],
            interpolation=interpolation,
            fill=fill,
            center=[0, 0],
        )
        label = F.affine(
            label,
            angle=0.0,
            translate=[0, 0],
            scale=1.0,
            shear=[math.degrees(math.atan(magnitude)), 0.0],
            interpolation=F.InterpolationMode.NEAREST,
            fill=255,
            center=[0, 0],
        )
    elif op_name == "ShearY":
        img = F.affine(
            img,
            angle=0.0,
            translate=[0, 0],
            scale=1.0,
            shear=[0.0, math.degrees(math.atan(magnitude))],
            interpolation=interpolation,
            fill=fill,
            center=[0, 0],
        )
        label = F.affine(
            label,
            angle=0.0,
            translate=[0, 0],
            scale=1.0,
            shear=[0.0, math.degrees(math.atan(magnitude))],
            interpolation=F.InterpolationMode.NEAREST,
            fill=255,
            center=[0, 0],
        )
    ## debug and test code for Translate X
    # elif op_name == "TranslateX":
    #     # 画像の幅の30%の移動を適用
    #     width = img.size[-1]  # 画像の幅を取得
    #     fixed_magnitude = int(0.3 * width)  # 幅の30%を計算

    #     img = F.affine(
    #         img,
    #         angle=0.0,
    #         translate=[fixed_magnitude, 0],  # fixed_magnitude を使用
    #         scale=1.0,
    #         interpolation=interpolation,
    #         shear=[0.0, 0.0],
    #         fill=fill,
    #     )
    #     label = F.affine(
    #         label,
    #         angle=0.0,
    #         translate=[fixed_magnitude, 0],  # 同じ値を使用
    #         scale=1.0,
    #         interpolation=F.InterpolationMode.NEAREST,
    #         shear=[0.0, 0.0],
    #         fill=255,
    #     )
    elif op_name == "TranslateX":
        img = F.affine(
            img,
            angle=0.0,
            translate=[int(magnitude), 0],
            scale=1.0,
            interpolation=interpolation,
            shear=[0.0, 0.0],
            fill=fill,
        )
        label = F.affine(
            label,
            angle=0.0,
            translate=[int(magnitude), 0],
            scale=1.0,
            interpolation=F.InterpolationMode.NEAREST,
            shear=[0.0, 0.0],
            fill=255,
        )
    # # debug and test for Translate Y
    # elif op_name == "TranslateY":
    #     height = img.size[-2]
    #     fixed_mag = int(height * 0.3)
    #     img = F.affine(
    #         img,
    #         angle=0.0,
    #         translate=[0, fixed_mag],
    #         scale=1.0,
    #         interpolation=interpolation,
    #         shear=[0.0, 0.0],
    #         fill=fill,
    #     )
    #     label = F.affine(
    #         label,
    #         angle=0.0,
    #         translate=[0, fixed_mag],
    #         scale=1.0,
    #         interpolation=F.InterpolationMode.NEAREST,
    #         shear=[0.0, 0.0],
    #         fill=255,
    #     )
    elif op_name == "TranslateY":
        img = F.affine(
            img,
            angle=0.0,
            translate=[0, int(magnitude)],
            scale=1.0,
            interpolation=interpolation,
            shear=[0.0, 0.0],
            fill=fill,
        )
        label = F.affine(
            label,
            angle=0.0,
            translate=[0, int(magnitude)],
            scale=1.0,
            interpolation=F.InterpolationMode.NEAREST,
            shear=[0.0, 0.0],
            fill=255,
        )
    elif op_name == "Rotate":
        img = F.rotate(img, magnitude, interpolation=interpolation, fill=fill)
        label = F.rotate(label, magnitude, interpolation=F.InterpolationMode.NEAREST, fill=255)
    elif op_name == "Brightness":
        magnitude = 2.0
        img = F.adjust_brightness(img, 1.0 + magnitude)
        # # print(f"Applied {op_name} with magnitude {magnitude}")  
        # brightness_factor = max(0.5, min(1.5, 1.0 + magnitude))  # 0.5から1.5の範囲に制限
        # img = F.adjust_brightness(img, brightness_factor)
        # ラベルには適用しない
    elif op_name == "Color":
        img = F.adjust_saturation(img, 1.0 + magnitude)
        # ラベルには適用しない
    elif op_name == "Contrast":
        img = F.adjust_contrast(img, 1.0 + magnitude)
        # ラベルには適用しない
    elif op_name == "Sharpness":
        # magnitude = 5.0
        img = F.adjust_sharpness(img, 1.0 + magnitude)
        # ラベルには適用しない
    elif op_name == "Posterize":
        # magnitude = 2.0
        img = F.posterize(img, int(magnitude))
        # ラベルには適用しない
    elif op_name == "Solarize":
        img = F.solarize(img, magnitude)
        # ラベルには適用しない
    elif op_name == "AutoContrast":
        img = F.autocontrast(img)
        # ラベルには適用しない
    elif op_name == "Equalize":
        img = F.equalize(img)
        # ラベルには適用しない
    # FIXME: NOT applied
    elif op_name == "Invert":
        
        # original_pix = np.array(img)
        # print(f"orignal pixel: {original_pix}")
        
        img = ImageOps.invert(img)
        
        # inverted_pix = np.array(img)
        # print(f"inverted pix: {inverted_pix}")
        

        # pix_diff = np.abs(original_pix - inverted_pix)
        # print(f"Average pixel difference: {np.mean(pix_diff)}")
        
        # output_dir = "./output_debug_invert"
        # if not os.path.exists(output_dir):
        #     os.makedirs(output_dir)
        
        # # ヒストグラムを保存
        # plt.figure(figsize=(15, 5))
        # plt.subplot(1, 2, 1)
        # plt.title('Histogram of Original Image')
        # plt.hist(original_pix.flatten(), bins=256, color='blue', alpha=0.5, label='Original')
        # plt.xlabel('Pixel Value')
        # plt.ylabel('Frequency')
        # plt.legend()
        # original_hist_path = os.path.join(output_dir, 'original_histogram.png')
        # plt.savefig(original_hist_path)

        # plt.subplot(1, 2, 2)
        # plt.title('Histogram of Inverted Image')
        # plt.hist(inverted_pix.flatten(), bins=256, color='red', alpha=0.5, label='Inverted')
        # plt.xlabel('Pixel Value')
        # plt.ylabel('Frequency')
        # plt.legend()
        # inverted_hist_path = os.path.join(output_dir, 'inverted_histogram.png')
        # plt.savefig(inverted_hist_path)

        # plt.show()
        # ラベルには適用しない
    elif op_name == "Identity":
        pass
    elif op_name == "Cutout":
        _, height, width = F.get_dimensions(img)
        cutout = Cutout(n_holes=1, img_size=min(height, width), patch_size=magnitude)
        sample = cutout({'image': img, 'label': label})
        img, label = sample['image'], sample['label']
    elif op_name == "Cutout_img": # Labelには適応させずに画像だけ適応させる
        _, height, width = F.get_dimensions(img)
        cutout = Cutout(n_holes=1, img_size=min(height, width), patch_size=magnitude)
        sample = cutout({'image' : img})
        img = sample['image']
    elif op_name == "SolarizeAdd":
        img = solarize_add(image=img, addition=int(magnitude), threshold=128)
        # ラベルには適用しない
    # elif op_name == "Hflip":
    #     img = ImageOps.mirror(img)
    #     label = ImageOps.mirror(label)
    # elif op_name == "Vflip":
    #     img = F.vflip(img)
    #     label = F.vflip(label)
        
    else:
        raise ValueError(f"The provided operator {op_name} is not recognized.")
    
    return {'image': img, 'label': label}


# copy and paste from original code
# TODO: if necessary, change this code into segmentaion version
class DefineAugmentSpace(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        
    def _ra_augmentation_space(self, num_bins: int, image_size: Tuple[int, int]) -> Dict[str, Tuple[Tensor, bool]]:
        space_dict = {
            # op_name: (magnitudes, signed)
            "Identity": (torch.tensor([0.0]), False),
            "ShearX": (torch.linspace(0.0, 0.3, num_bins), True),
            "ShearY": (torch.linspace(0.0, 0.3, num_bins), True),
            "TranslateX": (torch.linspace(0.0, 10.0, num_bins), True),
            "TranslateY": (torch.linspace(0.0, 10.0, num_bins), True),
            "Rotate": (torch.linspace(0.0, 30.0, num_bins), True),
            "Brightness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Color": (torch.linspace(0.1, 1.9, num_bins), True),
            "Contrast": (torch.linspace(0.1, 1.9, num_bins), True),
            "Sharpness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Posterize": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(), False),
            "Solarize": (torch.linspace(255.0, 0.0, num_bins), False),
            "AutoContrast": (torch.tensor([0.0]), False),
            "Equalize": (torch.tensor([0.0]), False),
            "Cutout": (torch.linspace(0.0, 0.5, num_bins), False),
            "SolarizeAdd": (torch.linspace(0, 110.0, num_bins), False),
            "Invert": (torch.tensor([0.0]), False),
            "Hflip":(torch.tensor([0.0]), False),
            "Vflip":(torch.tensor([0.0]), False)
        }
        if image_size[0] > 100:
            space_dict["TranslateX"] = (torch.linspace(0.0, 100.0, num_bins), True)
            space_dict["TranslateY"] = space_dict["TranslateX"]

        return space_dict


    def _ra_wide_augmentation_space(self, num_bins: int, image_size: Tuple[int, int]) -> Dict[str, Tuple[Tensor, bool]]:
        return {
            # op_name: (magnitudes, signed)
            "Identity": (torch.tensor([0.0]), False),
            "ShearX": (torch.linspace(0.0, 0.99, num_bins), True),
            "ShearY": (torch.linspace(0.0, 0.99, num_bins), True),
            "TranslateX": (torch.linspace(0.0, 32.0, num_bins), True),
            "TranslateY": (torch.linspace(0.0, 32.0, num_bins), True),
            "Rotate": (torch.linspace(0.0, 135.0, num_bins), True),
            "Brightness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Color": (torch.linspace(0.1, 1.9, num_bins), True),
            "Contrast": (torch.linspace(0.1, 1.9, num_bins), True),
            "Sharpness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Posterize": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 6)).round().int(), False),
            "Solarize": (torch.linspace(255.0, 0.0, num_bins), False),
            "AutoContrast": (torch.tensor([0.0]), False),
            "Equalize": (torch.tensor([0.0]), False)
        }

    def _original_augmentation_space(self, num_bins: int, image_size: Tuple[int, int]) -> Dict[str, Tuple[Tensor, bool]]:
        space_dict = {
            # op_name: (magnitudes, signed)
            "Identity": (torch.tensor([0.0]), False),
            "ShearX": (torch.linspace(0.0, 0.3, num_bins), True),
            "ShearY": (torch.linspace(0.0, 0.3, num_bins), True),
            "TranslateX": (torch.linspace(0.0, 10, num_bins), True),
            "TranslateY": (torch.linspace(0.0, 10, num_bins), True),
            "Rotate": (torch.linspace(0.0, 30.0, num_bins), True),
            "Brightness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Color": (torch.linspace(0.1, 1.9, num_bins), True),
            "Contrast": (torch.linspace(0.1, 1.9, num_bins), True),
            "Sharpness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Posterize": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(), False),
            "Solarize": (torch.linspace(255.0, 0.0, num_bins), False),
            "AutoContrast": (torch.tensor([0.0]), False),
            "Equalize": (torch.tensor([0.0]), False),
            ###
            "Cutout": (torch.linspace(0.0, 0.5, num_bins), False),
            "SolarizeAdd": (torch.linspace(0, 110.0, num_bins), False),
            "Invert": (torch.tensor([0.0]), False),
            ###
            "Hflip":(torch.tensor([0.0]), False),
            "Vflip":(torch.tensor([0.0]), False),
        }
        
        return space_dict

    def _jda_augmentation_space(self, num_bins: int, image_size: Tuple[int, int]) -> Dict[str, Tuple[Tensor, bool]]:
        space_dict = {
            # op_name: (magnitudes, signed)
            "Identity": (torch.tensor([0.0]), False),
            "ShearX": (torch.linspace(0.0, 0.3, num_bins), True),
            "ShearY": (torch.linspace(0.0, 0.3, num_bins), True),
            "TranslateX": (torch.linspace(0.0, 10.0, num_bins), True),
            "TranslateY": (torch.linspace(0.0, 10.0, num_bins), True),
            "Rotate": (torch.linspace(0.0, 30.0, num_bins), True),
            "Brightness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Color": (torch.linspace(0.1, 1.9, num_bins), True),
            "Contrast": (torch.linspace(0.1, 1.9, num_bins), True),
            "Sharpness": (torch.linspace(0.1, 1.9, num_bins), True),
            "Posterize": (8 - (torch.arange(num_bins) / ((num_bins - 1) / 4)).round().int(), False),
            "Solarize": (torch.linspace(255.0, 0.0, num_bins), False),
            "AutoContrast": (torch.tensor([0.0]), False),
            "Equalize": (torch.tensor([0.0]), False),
            "Invert": (torch.tensor([0.0]), False),
        }
        if image_size[0] > 100:
            space_dict["TranslateX"] = (torch.linspace(0.0, 100.0, num_bins), True)
            space_dict["TranslateY"] = space_dict["TranslateX"]

        return space_dict

================
File: augment.py
================
import numpy as np
import numbers
import random
import torch
import torchvision.transforms.functional as F
from torchvision import transforms

from PIL import ImageOps, Image

class Normalize(object):
    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):
        self.mean = mean
        self.std = std

    def __call__(self, sample):
        img = sample['image']
        mask = sample['label']
        print(img.size())
        print(mask.size())
        img = np.array(img).astype(np.float32)
        mask = np.array(mask).astype(np.int64)  # ラベルはint64型に変換
        img /= 255.0
        img -= self.mean
        img /= self.std
       
        return {'image': img, 'label': mask}


# https://github.com/YutaroOgawa/pytorch_advanced/blob/master/3_semantic_segmentation/utils/data_augumentation.py
class Normalize_Tensor(object):
    def __init__(self, color_mean, color_std):
        self.color_mean = color_mean
        self.color_std = color_std

    def __call__(self, sample):
        img = sample['image']
        anno_class_img = sample['label']

        # PIL画像をTensorに。大きさは最大1に規格化される
        img = transforms.functional.to_tensor(img)

        # 色情報の標準化
        img = transforms.functional.normalize(
            img, self.color_mean, self.color_std)

        # アノテーション画像をNumpyに変換
        anno_class_img = np.array(anno_class_img)  # [高さ][幅]

        #### VOCの時この処理に注意　#####
        # 'ambigious'には255が格納されているので、0の背景にしておく
        index = np.where(anno_class_img == 255)
        anno_class_img[index] = 0

        # アノテーション画像をTensorに
        anno_class_img = torch.from_numpy(anno_class_img)

        return {'image': img, 'label': anno_class_img}
    


class ToTensor(object):
    def __call__(self, sample):
       
        img = sample['image']
        mask = sample['label']
        img = np.array(img).astype(np.float32).transpose((2, 0, 1))
        mask = np.array(mask).astype(np.float32)

        img = torch.from_numpy(img).float()
        mask = torch.from_numpy(mask).float()

        return {'image': img, 'label': mask}
    


class RandomCrop(object):
    def __init__(self, size, padding=0):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size # h, w
        self.padding = padding

    def __call__(self, sample):
        img, mask = sample['image'], sample['label']

        if self.padding > 0:
            img = F.pad(img, self.padding, fill=0)
            mask = F.pad(mask, self.padding, fill=255)

        # クロップ位置を乱数で固定
        i, j, h, w = transforms.RandomCrop.get_params(img, output_size=self.size)
    
        # 画像とラベルを同じ位置でクロップ    
        img = F.crop(img, i, j, h, w)
        mask = F.crop(mask, i, j, h, w)

        return {'image': img, 'label': mask}
    
    
def resize(sample, size, interpolation="bilinear", max_size=None, antialias=None):
    img, label = sample['image'], sample['label']
    
    def _resize_img(img, size, interpolation, max_size, antialias):
        return F.resize(img, size, interpolation=interpolation, max_size=max_size, antialias=antialias)
    
    def _resize_label(label, size, interpolation, max_size, antialias):
        return F.resize(label.unsqueeze(0), size, interpolation="nearest").squeeze(0)
    
    if isinstance(size, (int, float)):
        size = [size, size]
    
    img_resized = _resize_img(img, size, interpolation, max_size, antialias)
    label_resized = _resize_label(label, size, interpolation, max_size, antialias)
    
    return {'image': img_resized, 'label': label_resized}


class Resize(object):
    def _init__(self, size, interpolation="bilinear", max_size=None, antialias=None):
        self.size = size
        self.interpolation = interpolation
        self.max_size = max_size
        self.antialias = antialias

    def __call__(self, sample):
        return resize(sample, self.size, self.interpolation, self.max_size, self.antialias)


import numpy as np
from PIL import Image

class Cutout(object):
    def __init__(self, n_holes, img_size, patch_size):
        self.n_holes = n_holes
        self.length = int(img_size * patch_size)

    def __call__(self, sample):
        """
        Args:
            sample (dict): Dictionary containing 'image' and 'label'.
        Returns:
            dict: Dictionary with Cutout applied to both 'image' and 'label'.
        """
        img, label = sample['image'], sample['label']
        
        if isinstance(img, Image.Image):
            img = np.array(img)
        if isinstance(label, Image.Image):
            label = np.array(label)
        
        h, w = img.shape[:2]
        mask = np.ones((h, w), np.float32)

        for _ in range(self.n_holes):
            y = np.random.randint(h)
            x = np.random.randint(w)

            y1 = np.clip(y - self.length // 2, 0, h)
            y2 = np.clip(y + self.length // 2, 0, h)
            x1 = np.clip(x - self.length // 2, 0, w)
            x2 = np.clip(x + self.length // 2, 0, w)

            mask[y1:y2, x1:x2] = 0

        if img.ndim == 3:
            mask = np.expand_dims(mask, axis=2)
            mask = np.repeat(mask, img.shape[2], axis=2)
        
        img = img * mask
        label = label * mask[:,:,0]  # ラベルは2次元なので、maskの1チャンネルだけを使用

        if isinstance(img, np.ndarray):
            img = Image.fromarray(img.astype(np.uint8))
        if isinstance(label, np.ndarray):
            label = Image.fromarray(label.astype(np.uint8))

        return {'image': img, 'label': label}
    
def solarize_add(image, addition=0, threshold=128):
    image_array = np.array(image, dtype=np.int64)

    added_image = image_array + addition
    clipped_image = np.clip(added_image, 0, 255)

    # 指定された閾値未満のピクセル値の領域に対して加算, クリップ
    result_image = np.where(image_array < threshold, clipped_image, image_array)
    result_image = result_image.astype(np.uint8)

    return Image.fromarray(result_image)

================
File: dataloader.py
================
import numpy as np
import os
import torch
import random
import matplotlib.pyplot as plt

from PIL import Image, ImageOps, ImageFilter
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from pathlib import Path

from augment import Cutout, Normalize, ToTensor, Normalize_Tensor, RandomCrop
from ra import RandAugmentSegmentation
from load_dataset.city import MYDataset
from load_dataset.voc import VOCDataset, datapath_list
# cfg.default.dataset_dir :  (SGE_LOCAL_DIR) + dataset/


def get_dataloader(cfg):
    train_transform = get_composed_transform(cfg, "train")
    val_transform = get_composed_transform(cfg, "val")
    test_transform = get_composed_transform(cfg, "test")

    # train_dataset = MYDataset(dataset_path, split='train', transform=train_transform)
    # val_dataset = MYDataset(dataset_path, split='val', transform=val_transform)
    # test_dataset = MYDataset(dataset_path, split='test', transform=test_transform)

    # データセット作成

    if cfg.dataset.name == "voc":
        # path_2012 = "/homes/ypark/code/dataset/VOCdevkit/VOC2012/"
        # path_2007 = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc/VOCdevkit/VOC2007"

        # abciで回すために少し変えました．
        # path_2012 = cfg.default.dataset_dir + "VOCSBD/"
        # path_2007 = cfg.default.dataset_dir + "VOC2007/VOCdevkit/VOC2007/"
        
        path_train = cfg.default.dataset_dir + "train_aug/"
        path_val = cfg.default.dataset_dir + "val/"
        # path_test = cfg.default.dataset_dir + "test/"
        path_test = cfg.default.dataset_dir + "test_2007/"


        print(f"load train from : {path_train} \n load validation from : {path_val} \n load test from : {path_test}")

        train_img_list, train_anno_list, val_img_list, val_anno_list, test_img_list, test_anno_list = datapath_list(
            path_train=path_train,
            path_val=path_val,
            path_test=path_test
            )
        
        
        train_dataset = VOCDataset(train_img_list, train_anno_list, phase="train", transform=train_transform, img_size=cfg.dataset.resized_size)
        val_dataset = VOCDataset(val_img_list, val_anno_list, phase="val", transform=val_transform, img_size=cfg.dataset.resized_size)
        ##### 追記をお願いします ############
        test_dataset = VOCDataset(test_img_list, test_anno_list, phase="test", transform=test_transform, img_size=cfg.dataset.resized_size)

    print(f"train dataset len : {len(train_dataset)}")
    print(f"val dataset len : {len(val_dataset)}")
    print(f"test dataset len : {len(test_dataset)}")

    train_loader = DataLoader(
        train_dataset, 
        batch_size=cfg.learn.batch_size, 
        num_workers=cfg.default.num_workers, 
        shuffle=True,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset, 
        batch_size=cfg.learn.batch_size, 
        num_workers=cfg.default.num_workers, 
        shuffle=False,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset, 
        batch_size=cfg.learn.batch_size, 
        num_workers=cfg.default.num_workers, 
        shuffle=False,
        pin_memory=True
    )

    visualize_augmentations(cfg, train_dataset)

    return train_loader, val_loader, test_loader

def get_composed_transform(cfg, phase):
    transform_list = []

    if phase == "train":
        for aug_name in cfg.augment.name:
            if aug_name == "rcrop":
                transform_list.append(
                    RandomCrop(size=cfg.dataset.resized_size, padding=cfg.augment.hp.rcrop_pad)
                )
            elif aug_name == "hflip":
                transform_list.append(
                    lambda x: {
                        'image': ImageOps.mirror(x['image']),
                        'label': ImageOps.mirror(x['label'])
                    }
                )
                # 50%
                # transform_list.append(
                #     lambda x: {
                #         'image': x['image'].flip(-1) if random.random() < 0.5 else x['image'],
                #         'label': x['label'].flip(-1) if x['image'].flip(-1).equal(x['image']) else x['label']
                #     }
                # )
            elif aug_name == "cutout":
                transform_list.append(
                    transforms.RandomApply(
                        [lambda x: {'image': Cutout(n_holes=1, img_size=cfg.dataset.resized_size, patch_size=cfg.augment.hp.cutout_size)(x['image']),
                                    'label': x['label']}],
                        p=cfg.augment.hp.cutout_p
                    )
                )
            elif aug_name == "ra":
                transform_list.append(RandAugmentSegmentation(cfg=cfg, num_ops=cfg.augment.ra.num_op, magnitude=cfg.augment.ra.magnitude))
            elif aug_name == "nan":
                pass
            else:
                raise ValueError(f"Invalid Augment ... {aug_name}")
    
    if cfg.dataset.name == "voc":
        transform_list.append(Normalize_Tensor(color_mean=cfg.dataset.mean, color_std=cfg.dataset.std))
    else:
        transform_list.append(ToTensor())
        transform_list.append(Normalize(mean=cfg.dataset.mean, std=cfg.dataset.std))

    transform_list = transforms.Compose(transform_list)

    return transform_list

def get_voc_colormap():
    colormap = np.zeros((256, 3), dtype=int)
    ind = np.arange(256, dtype=int)

    for shift in reversed(range(8)):
        for channel in range(3):
            colormap[:, channel] |= ((ind >> channel) & 1) << shift
    return colormap

def visualize_label(label, colormap):
    r = label.copy()
    g = label.copy()
    b = label.copy()
    for l in range(0, len(colormap)):
        r[label == l] = colormap[l, 0]
        g[label == l] = colormap[l, 1]
        b[label == l] = colormap[l, 2]
    rgb = np.stack([r, g, b], axis=2)
    return rgb

def visualize_augmentations(cfg, train_dataset):
    output_dir = os.path.join(cfg.out_dir, "aug_samples")
    os.makedirs(output_dir, exist_ok=True)

    sample_indices = random.sample(range(len(train_dataset)), 5)
    
    def denormalize(tensor, mean, std):
        for t, m, s in zip(tensor, mean, std):
            t.mul_(s).add_(m)
        return tensor

    colormap = get_voc_colormap()

    for idx in sample_indices:
        original_image, original_label = train_dataset.pull_item(idx)
        
        aug_sample = train_dataset[idx]
        aug_image = aug_sample['image']
        aug_label = aug_sample['label']

        fig, axs = plt.subplots(2, 2, figsize=(12, 12))

        # Original image
        axs[0, 0].imshow(np.array(original_image).astype(np.uint8))
        axs[0, 0].set_title("Original Image")
        axs[0, 0].axis('off')

        # Original label
        original_label_rgb = visualize_label(np.array(original_label), colormap)
        axs[0, 1].imshow(original_label_rgb)
        axs[0, 1].set_title("Original Label")
        axs[0, 1].axis('off')

        # Augmented image
        denormalized_image = denormalize(aug_image.clone(), cfg.dataset.mean, cfg.dataset.std)
        denormalized_image = (denormalized_image.permute(1, 2, 0) * 255).clamp(0, 255).byte().numpy()
        axs[1, 0].imshow(denormalized_image)
        axs[1, 0].set_title("Augmented Image")
        axs[1, 0].axis('off')
        
        # Augmented label
        aug_label_rgb = visualize_label(aug_label.numpy(), colormap)
        axs[1, 1].imshow(aug_label_rgb)
        axs[1, 1].set_title("Augmented Label")
        axs[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"augmentation_sample_{idx}.png"))
        plt.close()

        print(f"Sample {idx}:")
        print(f"Original Image - Min: {np.min(original_image)}, Max: {np.max(original_image)}")
        print(f"Original Label - Min: {np.min(original_label)}, Max: {np.max(original_label)}")
        print(f"Augmented Image - Min: {torch.min(aug_image).item()}, Max: {torch.max(aug_image).item()}")
        print(f"Augmented Label - Min: {torch.min(aug_label).item()}, Max: {torch.max(aug_label).item()}")
        print("---")
        
    print(f"Augmentation samples saved to {output_dir}")

================
File: evalator.py
================
import torch
import numpy as np

class Evaluator(object):
    def __init__(self, num_class):
        self.num_class = num_class
        self.reset()

    def reset(self):
        self.total_area_inter = torch.zeros(self.num_class).cuda()
        self.total_area_union = torch.zeros(self.num_class).cuda()
        self.total_area_pred = torch.zeros(self.num_class).cuda()
        self.total_area_label = torch.zeros(self.num_class).cuda()

    def add_batch(self, predict, label):
        area_inter, area_union, area_pred, area_label = self.intersectionAndUnionGPU(predict, label, self.num_class)
        self.total_area_inter += area_inter
        self.total_area_union += area_union
        self.total_area_pred += area_pred
        self.total_area_label += area_label

    def intersectionAndUnionGPU(self, output, target, K, ignore_index=255):
        output = output.clone()
        target = target.clone()
        output[target == ignore_index] = ignore_index
        intersection = output[output == target]
        area_inter = torch.histc(intersection, bins=K, min=0, max=K-1)
        area_pred = torch.histc(output, bins=K, min=0, max=K-1)
        area_label = torch.histc(target, bins=K, min=0, max=K-1)
        area_union = area_pred + area_label - area_inter
        return area_inter, area_union, area_pred, area_label

    def Mean_Intersection_over_Union(self):
        MIoU = self.total_area_inter / (self.total_area_union + 1e-10)
        MIoU = MIoU.cpu().numpy()
        return np.mean(MIoU)

    # def Pixel_Accuracy(self):
    #     Acc = torch.sum(self.total_area_inter) / (torch.sum(self.total_area_label) + 1e-10)
    #     Acc = Acc.cpu().numpy()
    #     return Acc
    
    def Pixel_Accuracy(self):
        Acc = torch.sum(self.total_area_inter) / (torch.sum(self.total_area_pred) + 1e-10)
        Acc = Acc.cpu().numpy()
        return Acc

================
File: main.py
================
import os
from PIL import Image, ImageOps, ImageFilter
import numpy as np
import sys
import matplotlib.pyplot as plt
import torch
import tqdm
import time
import pandas as pd

from set_cfg import setup_config, add_config
from evalator import Evaluator
from dataloader import get_dataloader
from train_val import train, val, test
from utils.common import (
    setup_device,
    fixed_r_seed,
    get_time,
    plot_log,
    save_learner  
)
from utils.suggest import (
    suggest_network,
    suggest_loss_func,
    suggest_optimizer,
    suggest_scheduler
)

# Add the parent dir to the sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

def visualize_samples(dataloader, num_samples=5):
    samples = next(iter(dataloader))
    images, labels = samples['image'], samples['label']

    # color_palette = []
    # for i in range(20):
    #     color_palette.append([i, i, i])
    # color_palette = np.array(color_palette)

    for i in range(min(num_samples, len(images))):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
        
        # 画像の表示
        img = images[i].permute(1, 2, 0).numpy()
        img = (img - img.min()) / (img.max() - img.min())  # 正規化
        ax1.imshow(img)
        ax1.set_title("Input Image")
        
        # ラベルの表示
        label = labels[i].squeeze().numpy()  # チャンネル次元を削除
        # label = color_palette[label]    # カラーパレットに従ってRGBに変更
        # label = (label - label.min()) / (label.max() - label.min())
        # ax2.imshow(label, cmap='jet')  # カラーマップを使用
        ax2.imshow(label)
        ax2.set_title("Label")
        
        cur_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(cur_dir)

        save_dir = os.path.join(parent_dir, "output", "sample")
        os.makedirs(save_dir, exist_ok=True)

        file_path = os.path.join(save_dir, f"sample_visualization_{i}.png")
        
        plt.savefig(file_path)
        plt.close()


def main(cfg):
    device = setup_device(cfg)
    fixed_r_seed(cfg)

    model = suggest_network(cfg)
    model.to(device)

    optimizer = suggest_optimizer(cfg, model)
    scheduler = suggest_scheduler(cfg, optimizer)

    criterion = suggest_loss_func(cfg)
    criterion.to(device)

    train_loader, val_loader, test_loader = get_dataloader(cfg)

    evaluator = Evaluator(cfg.dataset.n_class)

    all_training_result = []
    start_time = time.time()
    best_miou = 0.0

    for epoch in range(1, cfg.learn.n_epoch+1):
        train_progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch}/{cfg.learn.n_epoch} [Train]')
        train_loss, train_mIoU, train_Acc = train(cfg, device, model, train_progress_bar, optimizer, criterion, evaluator, epoch)
    
        val_progress_bar = tqdm.tqdm(val_loader, desc=f'Epoch {epoch}/{cfg.learn.n_epoch} [Val]')
        val_loss, val_mIoU, val_Acc = val(cfg, device, model, val_progress_bar, criterion, evaluator, epoch)

        all_training_result.append({
            "epoch": epoch, 
            "train_loss": train_loss, 
            "train_mIoU": train_mIoU, 
            "train_acc": train_Acc,
            "val_loss": val_loss,
            "val_mIoU": val_mIoU, 
            "val_acc": val_Acc
        })

        epoch_end_time = time.time()
        total_duration = get_time(epoch_end_time - start_time)

        print(f"{total_duration}, lr : {optimizer.param_groups[0]['lr']}")
        print(f"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_Acc:.4f}, Train mIoU: {train_mIoU:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_Acc:.4f}, Val mIoU: {val_mIoU:.4f}")
        print("-" * 80)

        if val_mIoU > best_miou:
            best_miou = val_mIoU
            print(f"New best mIoU: {best_miou}. Saving model...")
            save_learner(cfg, model, device, True)
            
        scheduler.step()

    end_time = time.time()
    total_training_time = get_time(end_time - start_time)
    print(f"Total training {total_training_time}")
    
    best_model_path = cfg.out_dir + "weights/best.pth"
    model.load_state_dict(torch.load(best_model_path))

    test_mIoU, test_Acc = test(cfg, device, model, test_loader, criterion)
    print(f"Final Test Results - Test Accuracy: {test_Acc:.4f}, Test mIoU: {test_mIoU:.4f}")

    test_result = {"test_mIoU": test_mIoU, "test_Acc": test_Acc}

    if len(all_training_result) > 0:
        train_df = pd.DataFrame(all_training_result)
        train_df.to_csv(cfg.out_dir + "train_output.csv", index=False)
        plot_log(cfg, train_df)

    test_df = pd.DataFrame([test_result])
    test_df.to_csv(cfg.out_dir + "test_output.csv", index=False)

    print(f"Train results saved to: {cfg.out_dir}train_output.csv")
    print(f"Test results saved to: {cfg.out_dir}test_output.csv")

    add_config(cfg, {"test_acc": float(test_Acc), "test_mIoU": float(test_mIoU)})
    add_config(cfg, {"total_training_time": str(total_training_time['time'])})

if __name__ == "__main__":
    cfg = setup_config()
    main(cfg)

================
File: notify.py
================
import requests
import sys

def main():
    url = "https://notify-api.line.me/api/notify"
    token = "UkmPbBk5nr2OrWnMFWXuygeOdHRhqeXP1hFyEDstCIN"
    headers = {"Authorization" : "Bearer "+ token}

    args = sys.argv
    if args[0] == 1:
        message =  '通常終了'
    else :
        message = '異常終了'
    payload = {"message" :  message}
    # files = {"imageFile": open("end.jpg", "rb")}

    r = requests.post(url ,headers = headers ,params=payload)

if __name__ == '__main__':
    main()

================
File: ra.py
================
import random
import torch
import os
import torchvision.transforms.functional as F
import numpy as np
import pandas as pd
from torch import Tensor, nn
from omegaconf import OmegaConf
from typing import List, Dict, Optional, Tuple

from aug_meta import DefineAugmentSpace, _apply_op
from set_cfg import override_original_config

# TODO: w/ affinity dataloader.py & affinity.pyにRandAugmentSegmentationクラスを元のコードを参考にして継承させる

def reset_cfg(cfg, init: bool):
    if init:
        if cfg.augment.ra.init_epoch is None:
            raise ValueError("Error... set num of init phase epoch")
        print(f"Set cfg for init phase, num of init phase epoch ...{cfg.augment.ra.init_epoch}")

        if cfg.augment.name[0] == "w_ra":
            cfg.augment.ra.warmup_ra = True
            print("Apply warmup RA")
    
        cfg.augment.name = ["nan"]
        cfg.save.affinity = True

    else:
        print("Set cfg for main phase")
        cfg.augment.name=["ra"]
        cfg.augment.ra.weight="affinity"
        cfg.save.affinity=False

        if cfg.augment.ra.warmup_ra:
            print("Set for warmup RA, weight is random")
            cfg.augment.ra.weight="random"

    override_original_config(cfg)
    print(OmegaConf.to_yaml(cfg))
    return cfg

class RandAugmentSegmentation(torch.nn.Module):
    def __init__(
        self,
        cfg,
        num_ops: int = 2,
        magnitude: int = 9,
        num_magnitude_bins: int = 31,
        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,
        fill: Optional[List[float]] = None,
    ) -> None:
        super().__init__()

        self.cfg = cfg
        self.num_ops = num_ops
        self.magnitude = magnitude
        self.num_magnitude_bins = num_magnitude_bins
        self.interpolation = interpolation
        self.fill = fill

        self.weight_type = cfg.augment.ra.weight
        self.softmax_t = cfg.augment.ra.softmax_t

        self.single = cfg.augment.ra.single

        self.space = DefineAugmentSpace()
        
        if self.cfg.augment.ra.space == "ra":
            self.op_meta = self.space._ra_augmentation_space(self.num_magnitude_bins, (32, 32))
        elif self.cfg.augment.ra.space == "jda":
            self.op_meta = self.space._jda_augmentation_space(self.num_magnitude_bins, (32, 32))

        self.weight = {key: 0 for key in self.op_meta.keys()}

        if "ra" in self.cfg.augment.name:
            if self.weight_type == "affinity":
                self.metrics_value = self.get_metrics_values(self.weight)

        self.weight = self.get_weight(self.weight)

        self.iden_rate = float(1 / len(self.op_meta))

        self.count = 0
        self.count_dict = {key: 0 for key in self.op_meta.keys()}

    def forward(self, sample: Dict[str, Tensor]) -> Dict[str, Tensor]:
        self.count += 1
        img, label = sample['image'], sample['label'] # apply ra to both image and label
        
        fill = self.fill
        channels, height, width = F.get_dimensions(img)

        if isinstance(img, Tensor):
            if isinstance(fill, (int, float)):
                fill = [float(fill)] * channels
            elif fill is not None:
                fill = [float(f) for f in fill]

         # aug spaceの設定
        if self.cfg.augment.ra.space == "ra":
            op_meta = self.space._ra_augmentation_space(self.num_magnitude_bins, (height, width))
        elif self.cfg.augment.ra.space == "jda":
            op_meta = self.space._jda_augmentation_space(self.num_magnitude_bins, (height, width))

        # 指定された回数だけ拡張操作を適用
        for _ in range(self.num_ops):
            # 重みに基づいて拡張操作を選択
            op_index = torch.multinomial(torch.tensor(list(self.weight.values())), 1, replacement=True).item()
            op_name = list(self.op_meta.keys())[op_index]
            # print(f"Selected operation: {op_name}")
            mag_range, signed = self.op_meta[op_name]

            # ランダムマグニチュードが有効な場合、マグニチュードをランダムに選択
            if self.cfg.augment.ra.random_magnitude:
                self.magnitude = torch.randint(len(mag_range), (1,), dtype=torch.long)

            # 選択されたマグニチュードを取得
            selected_mag = float(mag_range[self.magnitude].item()) if len(mag_range) > 1 else 0.0
            
            # 符号付きの場合、50%の確率で符号を反転
            if signed and torch.randint(2, (1,)):
                selected_mag *= -1.0

            # 重み付け方法に応じて拡張操作を適用
            if self.weight_type == "affinity":
                if random.random() < self.iden_rate:
                    op_name = list(self.op_meta.keys())[0]  # Identity
                else:
                    augmented_sample = _apply_op({'image': img, 'label': label}, op_name, selected_mag, interpolation=self.interpolation, fill=fill)
                    img, label = augmented_sample['image'], augmented_sample['label']
            else:
                augmented_sample = _apply_op({'image': img, 'label': label}, op_name, selected_mag, interpolation=self.interpolation, fill=fill)
                img, label = augmented_sample['image'], augmented_sample['label']

            self.count_dict[op_name] += 1

        # 一定間隔で選択された拡張操作の履歴を保存
        if self.count % self.cfg.learn.batch_size == 0:
            if self.cfg.save.selected:
                self.save_history()

        # 拡張された画像とラベルを返す
        return {'image': img, 'label': label}
    
    def save_history(self):
        pass

    def __repr__(self) -> str:
        # クラスの文字列表現を返す
        return (
            f"{self.__class__.__name__}("
            f"num_ops={self.num_ops}, "
            f"magnitude={self.magnitude}, "
            f"num_magnitude_bins={self.num_magnitude_bins}, "
            f"interpolation={self.interpolation}, "
            f"fill={self.fill}"
            f")"
        )

    def get_weight(self, weight):
        # 重み付け方法に応じて重みを計算
        if self.weight_type == "random":
            # ランダムな場合、全ての操作に等しい重みを与える
            weight_value = np.ones(len(weight))
            weight_value = weight_value / sum(weight_value)

        elif self.weight_type == "single":
            weight[self.single] = 1.0
            weight_value = np.array(list(weight.values()))

        else:
            weight_value = self.metrics_value

            if self.cfg.augment.ra.fix_iden:
                weight_value = np.delete(weight_value, 0)
                weight_value = nn.functional.softmax(
                    torch.tensor(weight_value) / self.cfg.augment.ra.softmax_t,
                    dim=0
                    )
                weight_value = torch.cat([torch.tensor([0,0], dtype=torch.float64), weight_value])

            else:
                weight_value = nn.functional.softmax(
                    torch.tensor(weight_value) / self.cfg.augment.ra.softmax_t,
                    dim=0
                )

        # 計算された重みを各操作に割り当てる
        for i, key in enumerate(weight):
            weight[key] = weight_value[i]

        return weight
    
    def get_metrics_values(self, weight):
        file_path = (self.cfg.out_dir + "affinity.csv")

        if not os.path.exists(file_path):
            if self.cfg.augment.ra.affinity_path is None:
                raise ValueError("affinity.csv path not found...")
            else:
                file_path = self.cfg.augment.ra.affinity_path

        df = pd.read_csv(file_path)
        for key in weight:
            weight[key] = df.iloc[-1][key]

        values = np.array(list(weight.values()))

        return values

================
File: ra_temp.py
================
from typing import List, Optional
import numpy as np
import random
import pandas as pd
from omegaconf import OmegaConf
import os
from pathlib import Path

import torch
from torch import Tensor, nn
import torchvision.transforms.functional as F

from aug_meta import DefineAugmentSpace, _apply_op
from set_cfg import override_original_config


# single w_ra


# これはSingle-pass Affinity-weighted RA用
# single-passでは学習途中に nan-aug -> Affinity RA に切り替わるので
# そのためにconfigを設定し直す
# 一旦今は無視してok

# for single-pass method
def reset_cfg(cfg, init: bool):
    # set for init phase
    if init:
        if cfg.augment.ra.init_epoch is None:
            raise ValueError("error.. set num of init phase epoch")
        print(f"Set cfg for init phase, num of init phase epoch ...{cfg.augment.ra.init_epoch}")

        if cfg.augment.name[0] == "w_ra":
            cfg.augment.ra.warmup_ra = True
            print("Apply warmup RA")

        cfg.augment.name=["nan"]
        cfg.save.affinity=True

    # set for main phase
    # start RA during training
    else:
        print("Set cfg for main phase")
        cfg.augment.name=["ra"]
        cfg.augment.ra.weight="affinity"
        cfg.save.affinity=False

        if cfg.augment.ra.warmup_ra:
            print("Set for warmup RA, weight is random")
            cfg.augment.ra.weight="random"

    override_original_config(cfg)
    print(OmegaConf.to_yaml(cfg))
    return cfg




class RandAugment(torch.nn.Module):
    def __init__(
        self,
        cfg,
        num_ops: int = 2,
        magnitude: int = 9,
        num_magnitude_bins: int = 31,
        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,
        fill: Optional[List[float]] = None,
    ) -> None:
        
        super().__init__()

        self.cfg = cfg

        # 1つの画像に連続して適用するDAの数　(基本は2)
        self.num_ops = num_ops 

        # 適用されるDAの強度　（基本はrandom magなのであとで画像毎にrandom samplingされる）
        self.magnitude = magnitude

        # magのレベルが何段階か ex)31段階中のmag=9に設定
        self.num_magnitude_bins = num_magnitude_bins

        # 画像の補完法について，気にしなくてok
        self.interpolation = interpolation
        self.fill = fill

        # 重み付けの手法 (3種類)
        # "random"　選択確率は全ての手法で等しい，普通のRA
        # "affinity"　affで重み付け
        # "single"　どれか1つの手法のみしか選択しなくなる
        self.weight_type = cfg.augment.ra.weight

        # 温度付きsoftmaxのtemperture paramerter
        # 小さいほど重みの制約は強く，大きいほどrandom性は増す
        # とりあえず softmax_t = 0.5 でやってみてください
        self.softmax_t = cfg.augment.ra.softmax_t

        # weight_typeが "single"である時に使う
        # if cfg.augment.ra.weight == "single" and cfg.augment.ra.single == "TranslateX"
        #  -> RAは"TranslateX"しか選択しなくなる
        self.single = cfg.augment.ra.single

        # RAが選択可能なDA手法のspace，どの先行研究の設定に合わせるか
        self.space = DefineAugmentSpace()

        # weightを取得するためにまず使用するRA spaceのDA手法名を取得，画像サイズは何でも良い
        if self.cfg.augment.ra.space == "ra":
            op_meta = self.space._ra_augmentation_space(self.num_magnitude_bins, (32, 32))
        elif self.cfg.augment.ra.space == "jda":
            op_meta = self.space._jda_augmentation_space(self.num_magnitude_bins, (32, 32))

        # 空のweightを用意
        self.weight = {key: 0 for key in op_meta.keys()}

        if "ra" in self.cfg.augment.name:
            # もしweight_typeがaffinityならcsvファイルから各DA手法のAffinity valuesを取得
            if self.weight_type == "affinity":
                self.metrics_value = self.get_metrics_value(self.weight)

            # weight valueを取得
            self.weight = self.get_weight(self.weight)
            
            # print("Weight values ...")
            # print(self.weight)
            
        # if fix Identity rate
        # Identity(無変換)の選択確率は重み付けせずに，一定確率で固定するために
        self.iden_rate = float(1 / len(op_meta))

        # count num of selected method
        # どのDA手法が何回countされたのかiteration毎に記録するためのdictを用意
        self.count = 0
        self.count_dict = {key: 0 for key in op_meta.keys()}



    def forward(self, img: Tensor) -> Tensor:
        self.count += 1
        fill = self.fill
        channels, height, width = F.get_dimensions(img)

        # 画像補完系の設定
        if isinstance(img, Tensor):
            if isinstance(fill, (int, float)):
                fill = [float(fill)] * channels
            elif fill is not None:
                fill = [float(f) for f in fill]

        # aug spaceの設定
        if self.cfg.augment.ra.space == "ra":
            op_meta = self.space._ra_augmentation_space(self.num_magnitude_bins, (height, width))
        elif self.cfg.augment.ra.space == "jda":
            op_meta = self.space._jda_augmentation_space(self.num_magnitude_bins, (height, width))
     
        # softmax_t is random smpled for each image
        # もしsoftmax_tを画像毎にrandom samplingしたかったら，
        # 基本使わないので，気にしないでok
        if self.cfg.augment.ra.softmax_t == "random":
            self.cfg.augment.ra.softmax_t = np.random.rand()
            self.weight = self.get_weight(self.weight)

        # apply transform num_ops times
        for _ in range(self.num_ops):
            # sample op according to weight value
            # weightに従って適用するDA手法のindexをサンプリング
            op_index = torch.multinomial(torch.tensor(list(self.weight.values())), 1, replacement=True).item()
            # サンプリングされたindexに対応するDA手法名を取得
            op_name = list(op_meta.keys())[op_index]
            # op_metaを参考にそのDA手法のmagnitude rangeとsinged(反転するかどうか)を取得
            # signed : 例えばrotateはmag rageが0~30度(右回転)に設定されているが 逆回転の0~(-30)度()左回転も考えられる
            # このように反対のハイパラ設定も考えられる場合はsiged = True，ある方向の変換だけでなく，反対方向に変換もできるように
            mag_range, signed = op_meta[op_name]

            # magnitudeがrandomの場合，適用するたびにmagnitudeをrandomにサンプリング
            if self.cfg.augment.ra.random_magnitude:
                self.magnitude = torch.randint(len(mag_range), (1,), dtype=torch.long)

            # transformのhyper-paraにrangがあるならlevelを指定，なかったら0.0を返す
            # self.magitudeの値を元に実際に適用するDA手法のハイパラ値を設定
            selected_mag = (float(mag_range[self.magnitude].item()) if len(mag_range) > 1 else 0.0)
            #magnitude = float(magnitudes[self.magnitude].item()) if magnitudes.ndim > 0 else 0.0
            
            # if hyper-para can invert, invert at 50% prob
            # signed == Trueの場合
            if signed and torch.randint(2, (1,)):
                selected_mag *= -1.0

            # Affinityで重み付けしてapply probを決めるが，
            if self.weight_type == "affinity":
                # for a certain prob -> Identity
                # self.iden_rateで決めた一定確率で必ずIdentity(無変換)
                if random.random() < self.iden_rate:
                    op_name = list(op_meta.keys())[0]
                # else, apply transform
                else:
                    img = _apply_op(img, op_name, selected_mag, interpolation=self.interpolation, fill=fill)
            else:
                img = _apply_op(img, op_name, selected_mag, interpolation=self.interpolation, fill=fill)

            # count selected transform
            # 選ばれたDA手法をカウント
            self.count_dict[op_name] += 1

        # たまに保存
        # 一定のiterationが経過したら保存
        if self.count % self.cfg.learn.batch_size == 0:
            if self.cfg.save.selected:
                self.save_history()

        return img
    
    # selected_countの保存
    # abciの場合SGD_LOCAL_DIRにあるファイルに書き出していく（今度説明します）
    def save_history(self):
        # for abci, save at SGE dir (temp)
        if self.cfg.default.env=="abci":
            sge_dir = str(Path(self.cfg.default.dataset_dir).parent)
            file_path = os.path.join(sge_dir, f"selected_method_{self.weight_type}.csv")
        else:
            file_path = self.cfg.out_dir + f"selected_method_{self.weight_type}.csv"

        df = pd.DataFrame([list(self.count_dict.values())], columns= list(self.count_dict.keys()))  

        for key in self.count_dict:
            self.count_dict[key] = 0
        
        # 既存csvに書き足す
        if os.path.exists(file_path) and self.count != 128:
            with open(file_path, 'a') as f:
                df.to_csv(f, header=False, index=False)
        # 1番初めはcsvを作成
        else:
            df.to_csv(file_path, index = False)
        

    def __repr__(self) -> str:
        s = (
            f"{self.__class__.__name__}("
            f"num_ops={self.num_ops}"
            f", magnitude={self.magnitude}"
            f", num_magnitude_bins={self.num_magnitude_bins}"
            f", interpolation={self.interpolation}"
            f", fill={self.fill}"
            f")"
        )
        return s
    
    # 重みの計算
    # 空のweight dictを引数に取る
    def get_weight(self, weight):
        # original RA, select at random
        # 普通のRA，全てのDA手法は等しい確率で選択される
        if self.weight_type == "random":
            weight_value = np.ones(len(weight))
            weight_value = weight_value / sum(weight_value)

        # selct only one method
        # cfg.augment.ra.weight == "single"なら．
        # cfg.augment.ra.singleで指定した手法のみを選択
        elif self.weight_type == "single":
            weight[self.single] = 1.0
            weight_value = np.array(list(weight.values()))

        # proposed method, weight for selection prob
        # Affinity valueを温度付きsoftmax関数に入れる
        else:
            weight_value = self.metrics_value
            # not weight for iden, iden prob is fixed
            if self.cfg.augment.ra.fix_iden:
                weight_value = np.delete(weight_value, 0)
                weight_value = nn.functional.softmax(
                    torch.tensor(weight_value) / self.cfg.augment.ra.softmax_t,
                    dim=0
                    )
                weight_value = torch.cat([torch.tensor([0.0], dtype=torch.float64), weight_value])
                
            else:
                weight_value = nn.functional.softmax(
                    torch.tensor(weight_value) / self.cfg.augment.ra.softmax_t,
                    dim=0
                    )

        # 空のweight dictに計算されたweight valuesを入れて返す
        for i, key in enumerate(weight):
            weight[key] = weight_value[i]

        return weight
   
    # get affinity value from csv
    # affinity valueをcsvファイルから読み出す

    # 基本的には実行中の実験結果が保存されるout_dirからaffinity .csvを読み出すが，
    # もしそこにファイルがない場合は
    # cfg.augment.ra.affinity_pathで指定したファイルから読み出す

    # weightと同じ空のweight dictを受け取って
    # そこにあるDA手法のAffinity valuesを読み出して
    # np.arrayのaffinity vakuesを返す
    def get_metrics_value(self, weight):
        file_path = (self.cfg.out_dir + "affinity.csv")

        # if not exist affinity.csv in same dir
        if not os.path.exists(file_path):
            # if affinity.csv path is not directed
            if self.cfg.augment.ra.affinity_path is None:
                raise ValueError("affinity.csv path not found ...")
            else:
                file_path = self.cfg.augment.ra.affinity_path

        # load affinity value from csv
        df = pd.read_csv(file_path)
        for key in weight:
            weight[key] = df.iloc[-1][key]

        values = np.array(list(weight.values()))

        return values

================
File: set_cfg.py
================
import os
import sys
import numpy as np

from pathlib import Path


from omegaconf import OmegaConf
import datetime



def setup_config():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    # parent_dir : ['/homes/ypark/code/segmentation/']

    args = sys.argv
    print(f"args: {args}")

    current_dir = Path(__file__).resolve().parent
    conf_dir = current_dir / "conf"

    if len(args) > 1:
        config_file_name = args[1]
        print(f"config_file_name: {config_file_name}")
        config_file_path = conf_dir / f"{config_file_name}.yaml"
    else:
        config_file_name = "test"
        print("selected file is a test.yaml")
        config_file_path = conf_dir / "test.yaml"
    config_file_path = str(current_dir) + f"/conf/{config_file_name}.yaml"
    if os.path.exists(config_file_path):
        cfg = OmegaConf.load(config_file_path)
    else:
        raise "No YAML file !!!"
    
    # 実行のhome dirを設定
    if cfg.default.home_dir is None:
        cfg.default.home_dir = f"{parent_dir}/"
    # dataset dirを設定
    if cfg.default.dataset_dir is None:
        cfg.default.dataset_dir = os.path.join(parent_dir, "dataset")

    # コマンドラインで受け取った引数とconfig_fileの情報をmerge
    cfg = OmegaConf.merge(cfg, OmegaConf.from_cli(args_list=args[2:]))
    file_name = get_filename(cfg)
    if "out_dir" not in cfg:
        output_dir_path = (
            f"{cfg.default.home_dir}"
            +f"{cfg.default.output_dir}/"
            + f"seed{cfg.default.seed}/"
            + f"{config_file_name}/"
            +f"{file_name}/"
        )
    else:
        output_dir_path = f"{cfg.out_dir}"

    if cfg.default.make_dir:
        print(f"MAKE DIR {output_dir_path}")
        os.makedirs(output_dir_path, exist_ok=True)

    out_dir_comp = {"out_dir": output_dir_path}
    cfg = OmegaConf.merge(cfg, out_dir_comp)

    config_name_comp = {"execute_config_name": config_file_name}
    cfg = OmegaConf.merge(cfg, config_name_comp)

    config_name_comp = {"override_cmd": args[2:]}
    cfg = OmegaConf.merge(cfg, config_name_comp)

    dt = datetime.datetime.today()
    datetime_comp = {"datetime": dt.strftime('%Y-%m-%d %H:%M:%S.%f')}
    cfg = OmegaConf.merge(cfg, datetime_comp)

    with open(output_dir_path + "config.yaml", "w") as f:
        OmegaConf.save(cfg, f)
    return cfg


def add_config(cfg, config_name_comp: dict):
    config_file_path = cfg.out_dir + "config.yaml"
    if os.path.exists(config_file_path):
        print("### Add config")
        print(config_name_comp)
        for key, value in config_name_comp.items():
            if isinstance(value, np.float64):
                value = float(value)
            cfg[key] = value
        with open(config_file_path, "w") as f:
            OmegaConf.save(cfg, f)
        return cfg
    else:
        raise "No YAML file !!!"
    
def override_original_config(cfg):
    config_file_path = cfg.out_dir + "config.yaml"
    if os.path.exists(config_file_path):
        original_cfg = OmegaConf.load(config_file_path)
    else:
        raise ValueError(f"cfg file {config_file_path} not found") 
    OmegaConf.merge(original_cfg, cfg)
    with open(config_file_path, "w") as f:
        OmegaConf.save(cfg, f)
    print("### Override config.")


# 実行中の結果を保存するdir nameの決定
# 命名則 : 基本的には適用したaug name，RA関連だけ区別できるように詳しい設定
# cfg.default.add_filenameでdir nameを追加することができる
def get_filename(cfg):
    if cfg.default.add_filename is not None:
        file_name = cfg.default.add_filename
    else:
        file_name = ""
    
    
    for aug_name in cfg.augment.name:
        print(aug_name)
        if aug_name == "ra":
            file_name = f"{file_name}RA{cfg.augment.ra.num_op}"
            if cfg.augment.ra.weight == "random":
                file_name = f"{file_name}_Random"
            elif cfg.augment.ra.weight == "single":
                file_name = f"{file_name}_{cfg.augment.ra.single}"
            elif cfg.augment.ra.weight == "affinity":
                file_name = f"{file_name}_Affinity{cfg.augment.ra.softmax_t}"
            else:
                raise ValueError(f"Invalid RandAugment weight type... {cfg.augment.ra.weight}")
            
            if cfg.augment.ra.random_magnitude:
                file_name = f"{file_name}_Randmag"

            
        elif aug_name == "single":
            file_name = f"{file_name}SinglePass{cfg.augment.ra.softmax_t}_{cfg.augment.ra.init_epoch}"
            if cfg.augment.ra.random_magnitude:
                file_name = f"{file_name}_Randmag"

        elif aug_name == "w_ra":
            file_name = f"{file_name}WarmupRA{cfg.augment.ra.init_epoch}"
            if cfg.augment.ra.random_magnitude:
                file_name = f"{file_name}_Randmag"
        
        else:
            file_name = f"{file_name}{aug_name.capitalize()}"

        
    return file_name

================
File: test.py
================
import os
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from omegaconf import OmegaConf

from set_cfg import setup_config
from dataloader import get_dataloader
from utils.common import setup_device
from utils.suggest import suggest_network

def test(cfg, device, model, test_loader):
    model.eval()
    
    output_dir = os.path.join(cfg.test.result_dir, "final_result_segmentation")
    os.makedirs(output_dir, exist_ok=True)
    
    with torch.no_grad():
        for i, sample in enumerate(tqdm(test_loader, desc="Generating Predicitons!!")):
            image = sample['image'].to(device)
            
            output = model(image)
            pred = torch.argmax(output, dim=1)
            
            for j, pred_single in enumerate(pred):
                pred_img = Image.fromarray(pred_single.byte().cpu().numpy().astype(np.uint8))
                pred_img.save(os.path.join(output_dir, f"pred_{i*cfg.learn.batch_size+j}.png"))
    
    print(f"Predicitons saved in {output_dir}")

def main(cfg):
    device = setup_device(cfg)
    
    model = suggest_network(cfg)
    model.to(device)
    
    # Load best model in the all experiments
    # This is because we cannnot send the each experiment results according to the PASCAL VOC home page.
    ###############
    # Best Practice
    # The VOC challenge encourages two types of participation: (i) methods which are trained using only the provided "trainval" (training + validation) data; (ii) methods built or trained using any data except the provided test data, for example commercial systems. In both cases the test data must be used strictly for reporting of results alone - it must not be used in any way to train or tune systems, for example by runing multiple parameter choices and reporting the best results obtained.
    # If using the training data we provide as part of the challenge development kit, all development, e.g. feature selection and parameter tuning, must use the "trainval" (training + validation) set alone. One way is to divide the set into training and validation sets (as suggested in the development kit). Other schemes e.g. n-fold cross-validation are equally valid. The tuned algorithms should then be run only once on the test data.
    # In VOC2007 we made all annotations available (i.e. for training, validation and test data) but since then we have not made the test annotations available. Instead, results on the test data are submitted to an evaluation server.
    # Since algorithms should only be run once on the test data we strongly discourage multiple submissions to the server (and indeed the number of submissions for the same algorithm is strictly controlled), as the evaluation server should not be used for parameter tuning.
    # We encourage you to publish test results always on the latest release of the challenge, using the output of the evaluation server. If you wish to compare methods or design choices e.g. subsets of features, then there are two options: (i) use the entire VOC2007 data, where all annotations are available; (ii) report cross-validation results using the latest "trainval" set alone.
    ############### 
    
    best_model_path = cfg.test.best_model
    model.load_state_dict(torch.load(best_model_path))
    
    _, _, test_loader = get_dataloader(cfg)
    
    test(cfg, device, model, test_loader)
    

if __name__ == "__main__":
    cfg = setup_config()
    main(cfg)

================
File: train_val.py
================
import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import tqdm
from utils.common import AverageMeter, intersectionAndUnionGPU

def get_pred(y):
    if isinstance(y, torch.Tensor):
        out = y
    else:
        out = y[0]
    return torch.argmax(out, dim=1)

def visualize_results(cfg, epoch, image, label, pred, phase):
    debug_dir = os.path.join(cfg.out_dir, "debug")
    os.makedirs(debug_dir, exist_ok=True)

    for j in range(min(3, image.shape[0])):  # Visualize up to 3 samples
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
        ax1.imshow(image[j].cpu().permute(1, 2, 0))
        ax1.set_title("Input Image")
        ax2.imshow(label[j].cpu())
        ax2.set_title("True Label")
        ax3.imshow(pred[j].cpu())
        ax3.set_title("Prediction")
        plt.savefig(os.path.join(debug_dir, f"debug_sample_epoch{epoch}_{phase}_sample{j}.png"))
        plt.close()

def train(cfg, device, model, train_progress_bar, optimizer, criterion, evaluator, epoch):
    model.train()
    evaluator.reset()
    loss_meter = AverageMeter()

    for i, sample in enumerate(train_progress_bar):
        image, label = sample['image'].to(device), sample['label'].to(device)

        if label.dim() == 4:
            label = label.squeeze(1)
        
        y = model(image)
        loss = criterion(y, label.long())
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # メトリクスの計算
        pred = get_pred(y)
        evaluator.add_batch(pred, label)
        
        loss_meter.update(loss.item(), image.size(0))
        train_progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        if epoch % 100 == 0 and i == 0:
            visualize_results(cfg, epoch, image, label, pred, 'train')

    mIoU = evaluator.Mean_Intersection_over_Union()
    Acc = evaluator.Pixel_Accuracy()

    return loss_meter.avg, mIoU, Acc

def val(cfg, device, model, val_progress_bar, criterion, evaluator, epoch):
    model.eval()
    evaluator.reset()
    loss_meter = AverageMeter()
    
    for i, sample in enumerate(val_progress_bar):
        image, label = sample['image'].to(device), sample['label'].to(device)

        if label.dim() == 4:
            label = label.squeeze(1)
        
        label = label.long()

        with torch.no_grad():
            y = model(image)

        loss = criterion(y, label)
        loss_meter.update(loss.item(), image.size(0))
        pred = get_pred(y)
        
        evaluator.add_batch(pred, label)
        val_progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        if epoch % 100 == 0 and i == 0:  # Visualize first batch every 100 epochs
            visualize_results(cfg, epoch, image, label, pred, 'val')

    mIoU = evaluator.Mean_Intersection_over_Union()
    Acc = evaluator.Pixel_Accuracy()

    return loss_meter.avg, mIoU, Acc

def test(cfg, device, model, test_loader, criterion):
    model.eval()
    intersection_meter = AverageMeter()
    union_meter = AverageMeter()
    target_meter = AverageMeter()
    
    test_progress_bar = tqdm.tqdm(test_loader, desc='Testing')
    
    for sample in test_progress_bar:
        image, label = sample['image'].to(device), sample['label'].to(device)

        if label.dim() == 4:
            label = label.squeeze(1)
        
        label = label.long()

        with torch.no_grad():
            output = model(image)
        
        loss = criterion(output, label)
        pred = get_pred(output)
        
        intersection, union, target = intersectionAndUnionGPU(pred, label, cfg.dataset.n_class, cfg.dataset.ignore_label)
        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()
        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)
        
        test_progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)
    mIoU = np.mean(iou_class)
    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)
    
    print(f"Test Results - Accuracy: {allAcc:.4f}, mIoU: {mIoU:.4f}")
    return mIoU, allAcc

================
File: conf/test.yaml
================
default:
  env: "cotton"
  seed: 1
  output_dir: "output"
  home_dir: null
  dataset_dir: null
  deterministic: False
  parallel: False
  device_id: 0
  num_workers: 8
  num_excute: 4
  make_dir: True
  add_filename: null


network:
  name: "segnet"
  pretrained: False
  dropout_rate: 0.3


learn:
  n_epoch: 175
  batch_size: 1


augment: 
  name: 
   - nan
  hp:
    rcrop_pad: 4
    cutout_p: 0.5
    cutout_size: 0.5
    ra_p: 1.0
    
  ra:
    space: "ra"
    weight: single # 第一段階ではsingle
    single: Contrast # 第一段階では特定のDA選択
    num_op: 2
    magnitude:  14
    random_magnitude: True
    softmax_t: 0.5
    affinity_path: null
    fix_iden: True
    init_epoch: 20
    warmup_ra: False
    aff_calc: False
    aff_model: null

  
dataset:
  name: "CityScapes" 
  n_class: 34
  img_size: 128
  resized_size: 128
  train_size: null
  mean: 
    - 0.28689554
    - 0.32513303
    - 0.28389177
  std: 
    - 0.18696375
    - 0.19017339
    - 0.18720214
  ignore_label: 255
  

optimizer:
  name: SGD
  scheduler:
    name: poly
    step:
      - 60
      - 120
      - 160
    power: 0.9

  loss: 
    name: "ce"
    aux_weight: 0.4
    
  hp:
    lr: 0.001
    lr_min: 0.000001
    warmup_period: null
    warmup_init: 1e-5
    momentum: 0.9
    weight_decay: 5e-4
    
save:
  img: True
  plot: True
  selected: True
  affinity: False
  affinity_all: False
  interval: null

================
File: conf/test_memo.yaml
================
default:
  env: "cotton" # abci用
  seed: 1
  output_dir: "output" # output dir名の指定
  home_dir: "/homes/ypark/code/segmentation/"
  dataset_dir: "/homes/ypark/code/segmentation/dataset/"
  deterministic: False # モデル学習を早くする分再現率は下がる，基本的にFalseで良い
  parallel: False # 分散学習用, 基本的にFalse
  device_id: 0 # abci用
  num_workers: 8 
  num_excute: 4 
  make_dir: True # Falseにするとoutput dirが作られなくなる, 基本的にFalse
  add_filename: null # set_cfg.py def get_filenam()参照


network:
  name: "segnet"
  pretrained: False # 基本今はスクラッチを使うからTrue用の実装は未搭載
  dropout_rate: 0.3 # 今のsegnet modelにはdrop outはないが，後々使うかも


learn:
  n_epoch: 400
  batch_size: 5

# ここはaugmentの実装ができてから擦り合わせましょう
augment: 
  name: 
   - "base" # aug name : list
  # p: probability, どの確率でDAを適用するか
  hp:
    rcrop_pad: 4 #rcropの際にパディングする辺のピクセル数
    cutout_p: 0.5
    cutout_size: 0.5 #画像の何倍分をcutoutで隠すか
    ra_p: 1.0
    
  ra:
    space: "ra" # RAが選択可能なDA手法のspace, "ra" or "jda"が選択可能(基本"ra")
    weight: null # weight type, "affinity", "random", "single"が選択可能, ra_temp.py参照
    single: null # weight_type == "singel"の際に使用, ra_temp.py参照
    num_op: 2
    magnitude:  14
    random_magnitude: True
    softmax_t: 0.5
    affinity_path: null
    fix_iden: True # Identity(無変換)の選択買う率は重み付けせずに一定に固定するかどうか
    init_epoch: 20 # for single pass method
    warmup_ra: False # for single pass method
    aff_calc: False # 学習前に学習済みモデルを読み出してかくDA手法のAffinity vakuesを計算してaffinity.csvを用意するか
    aff_model: null #上の学習済みモデルのweight path

  

dataset:
  name: "CitySpace" 
  n_class: 41 # datasetのクラス数
  img_size: 128
  resized_size: 128 # リサイズ後の画像サイズ
  train_size: null # もし学習データの一部だけを使いたいときに..気にしなくて良い
  ### Normalizeのパラメータ，city用に合わせる必要あり
  mean: 
    - 0.485
    - 0.456
    - 0.406
  std: 
    - 0.229
    - 0.224
    - 0.225
  

optimizer:
  name: SGD
  scheduler:
    name: cosine
    # step lr schedulingを使う時に使用，cosineを使うので気にしない
    step:
      - 60
      - 120
      - 160

  loss: 
    name: "ce"
    aux_weight: 0.4
    
  hp:
    ### 設定し直してください
    lr: 0.1
    lr_min: 0.0001
    #lr schedulerにwarmupを使用する時に使用
    warmup_period: null 
    warmup_init: 1e-5
    # ptimizerのハイパラ
    momentum: 0.9
    weight_decay: 5e-4
    
# 今は気にしなくて大丈夫です
save:
  img: True 
  plot: True
  selected: True
  affinity: False
  affinity_all: False
  interval: null

================
File: conf/voc.yaml
================
default:
  env: "abci"
  seed: 105
  output_dir: "output"
  home_dir: null
  dataset_dir: null
  deterministic: False
  parallel: False
  device_id: 0
  num_workers: 16
  num_excute: 4
  make_dir: True
  add_filename: null


network:
  name: "pspnet"
  pretrained: True
  dropout_rate: 0.1
  zoom_factor: 8
  use_ppm: True
  resenet_layers: 50



learn:
  n_epoch: 330
  batch_size: 16


augment: 
  name: 
   - rcrop
  hp:
    rcrop_pad: 237
    # 画像サイズが475なことから
    # min: 475*0.5 = 237.5
    # max: 475*2 = 950
    # maxと元の画像の差: 950-475 = 475
    # 475/2 = 237.5
    # よって237pix

    cutout_p: 0.5
    cutout_size: 0.5
    ra_p: 1.0
    
  ra:
    space: "ra"
    weight: single # 第一段階ではsingle
    single: Contrast # 第一段階では特定のDA選択
    num_op: 2
    magnitude:  14
    random_magnitude: True
    softmax_t: 0.5
    affinity_path: null
    fix_iden: True
    init_epoch: 20
    warmup_ra: False
    aff_calc: False
    aff_model: null

  
dataset:
  name: "voc" 
  n_class: 21
  img_size: 465
  resized_size: 465
  train_size: null
  mean: 
    - 0.485
    - 0.456
    - 0.406
  std: 
    - 0.229
    - 0.224
    - 0.225
  ignore_label: 255
  

optimizer:
  name: SGD
  scheduler:
    name: poly
    step:
      - 60
      - 120
      - 160
    power: 0.9

  loss: 
    name: "ce"
    aux_weight: 0.4
    
  hp:
    lr: 0.01
    lr_min: 0.00001
    warmup_period: null
    warmup_init: 1e-5
    momentum: 0.9
    weight_decay: 1e-4
    
save:
  img: True
  plot: True
  selected: True
  affinity: False
  affinity_all: False
  interval: null

# test:
#   test_dataset: /hoge/fuga
#   best_model: /hoge/hoge/fuga/fuga
#   result_dir: /hoge/hoge/fuga/fuga

================
File: conf/voc_abci.yaml
================
default:
  env: "abci"
  seed: 1
  output_dir: "output"
  home_dir: null
  dataset_dir: null
  deterministic: False
  parallel: False
  device_id: 0
  num_workers: 8
  num_excute: 4
  make_dir: True
  add_filename: null


network:
  name: "segnet"
  pretrained: False
  dropout_rate: 0.3


learn:
  n_epoch: 175
  batch_size: 1


augment: 
  name: 
   - nan
  hp:
    rcrop_pad: 237
    # 画像サイズが475なことから
    # min: 475*0.5 = 237.5
    # max: 475*2 = 950
    # maxと元の画像の差: 950-475 = 475
    # 475/2 = 237.5
    # よって237pix
    cutout_p: 0.5
    cutout_size: 0.5
    ra_p: 1.0
    
  ra:
    space: "ra"
    weight: single # 第一段階ではsingle
    single: Contrast # 第一段階では特定のDA選択
    num_op: 2
    magnitude:  14
    random_magnitude: True
    softmax_t: 0.5
    affinity_path: null
    fix_iden: True
    init_epoch: 20
    warmup_ra: False
    aff_calc: False
    aff_model: null

  
dataset:
  name: "voc" 
  n_class: 21
  img_size: 47
  resized_size: 475
  train_size: null
  mean: 
    - 0.485
    - 0.456
    - 0.406
  std: 
    - 0.229
    - 0.224
    - 0.225
  ignore_label: 255
  

optimizer:
  name: SGD
  scheduler:
    name: poly
    step:
      - 60
      - 120
      - 160
    power: 0.9
    
  hp:
    lr: 0.01
    lr_min: 0.00001
    warmup_period: null
    warmup_init: 1e-5
    momentum: 0.9
    weight_decay: 1e-4
    
save:
  img: True
  plot: True
  selected: True
  affinity: False
  affinity_all: False
  interval: null

================
File: load_dataset/city.py
================
import os
from PIL import Image


from torch.utils.data import Dataset

def is_image(filename):
    return any(filename.endswith(ext) for ext in '.png')

def is_label(filename):
    return filename.endswith(".png")

def image_basename(filename):
    return os.path.basename(os.path.splitext(filename)[0])

class MYDataset(Dataset):
    
    def __init__(self, root, split, transform):
        self._base_dir = root
        self.split = split
        self.images_root = os.path.join(self._base_dir, split, 'rgb/')
        self.labels_root = os.path.join(self._base_dir, split, 'label/')
        
        self.filenames = [image_basename(f)
            for f in os.listdir(self.images_root) if is_image(f)]
        self.filenames.sort()
        self.filenamesGt = [image_basename(f)
            for f in os.listdir(self.labels_root) if is_label(f)]
        
        self.filenamesGt.sort()
        self.transform = transform

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, index):
         # 1. 画像読み込み
        image_file_path = self.filenames[index]+ '.png'
        image_file_path = os.path.join(self._base_dir, self.split, 'rgb/', image_file_path)
        img = Image.open(image_file_path).convert('RGB')

        # 2. アノテーション画像読み込み
        label_file_path = self.filenamesGt[index]+ '.png'
        label_file_path = os.path.join(self._base_dir, self.split, 'label/', label_file_path)
        label_class_img = Image.open(label_file_path).convert('L')  


        sample = {'image': img, 'label': label_class_img}

        # 3. データ拡張を実施
        if self.transform:
            sample = self.transform(sample)


        return sample

================
File: load_dataset/psp_data_augmentation.py
================
# 第3章セマンティックセグメンテーションのデータオーギュメンテーション
# 注意　アノテーション画像はカラーパレット形式（インデックスカラー画像）となっている。

# パッケージのimport
import torch
from torchvision import transforms
from PIL import Image, ImageOps, ImageFilter
import numpy as np


class Compose(object):
    """引数transformに格納された変形を順番に実行するクラス
       対象画像とアノテーション画像を同時に変換させます。 
    """

    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, img, anno_class_img):
        for t in self.transforms:
            img, anno_class_img = t(img, anno_class_img)
        return img, anno_class_img


class Scale(object):
    def __init__(self, scale):
        self.scale = scale

    def __call__(self, img, anno_class_img):

        width = img.size[0]  # img.size=[幅][高さ]
        height = img.size[1]  # img.size=[幅][高さ]

        # 拡大倍率をランダムに設定
        scale = np.random.uniform(self.scale[0], self.scale[1])

        scaled_w = int(width * scale)  # img.size=[幅][高さ]
        scaled_h = int(height * scale)  # img.size=[幅][高さ]

        # 画像のリサイズ
        img = img.resize((scaled_w, scaled_h), Image.BICUBIC)

        # アノテーションのリサイズ
        anno_class_img = anno_class_img.resize(
            (scaled_w, scaled_h), Image.NEAREST)

        # 画像を元の大きさに
        # 切り出し位置を求める
        if scale > 1.0:
            left = scaled_w - width
            left = int(np.random.uniform(0, left))

            top = scaled_h-height
            top = int(np.random.uniform(0, top))

            img = img.crop((left, top, left+width, top+height))
            anno_class_img = anno_class_img.crop(
                (left, top, left+width, top+height))

        else:
            # input_sizeよりも短い辺はpaddingする
            p_palette = anno_class_img.copy().getpalette()

            img_original = img.copy()
            anno_class_img_original = anno_class_img.copy()

            pad_width = width-scaled_w
            pad_width_left = int(np.random.uniform(0, pad_width))

            pad_height = height-scaled_h
            pad_height_top = int(np.random.uniform(0, pad_height))

            img = Image.new(img.mode, (width, height), (0, 0, 0))
            img.paste(img_original, (pad_width_left, pad_height_top))

            anno_class_img = Image.new(
                anno_class_img.mode, (width, height), (0))
            anno_class_img.paste(anno_class_img_original,
                                 (pad_width_left, pad_height_top))
            anno_class_img.putpalette(p_palette)

        return img, anno_class_img


class RandomRotation(object):
    def __init__(self, angle):
        self.angle = angle

    def __call__(self, img, anno_class_img):

        # 回転角度を決める
        rotate_angle = (np.random.uniform(self.angle[0], self.angle[1]))

        # 回転
        img = img.rotate(rotate_angle, Image.BILINEAR)
        anno_class_img = anno_class_img.rotate(rotate_angle, Image.NEAREST)

        return img, anno_class_img


class RandomMirror(object):
    """50%の確率で左右反転させるクラス"""

    def __call__(self, img, anno_class_img):
        if np.random.randint(2):
            img = ImageOps.mirror(img)
            anno_class_img = ImageOps.mirror(anno_class_img)
        return img, anno_class_img


class Resize(object):
    """引数input_sizeに大きさを変形するクラス"""

    def __init__(self, input_size):
        self.input_size = input_size

    def __call__(self, img, anno_class_img):

        # width = img.size[0]  # img.size=[幅][高さ]
        # height = img.size[1]  # img.size=[幅][高さ]

        img = img.resize((self.input_size, self.input_size),
                         Image.BICUBIC)
        anno_class_img = anno_class_img.resize(
            (self.input_size, self.input_size), Image.NEAREST)

        return img, anno_class_img


class Normalize_Tensor(object):
    def __init__(self, color_mean, color_std):
        self.color_mean = color_mean
        self.color_std = color_std

    def __call__(self, img, anno_class_img):

        # PIL画像をTensorに。大きさは最大1に規格化される
        img = transforms.functional.to_tensor(img)

        # 色情報の標準化
        img = transforms.functional.normalize(
            img, self.color_mean, self.color_std)

        # アノテーション画像をNumpyに変換
        anno_class_img = np.array(anno_class_img)  # [高さ][幅]

        # 'ambigious'には255が格納されているので、0の背景にしておく
        index = np.where(anno_class_img == 255)
        anno_class_img[index] = 0

        # アノテーション画像をTensorに
        anno_class_img = torch.from_numpy(anno_class_img)

        return img, anno_class_img

================
File: load_dataset/voc.py
================
import os
import urllib.request
import tarfile
import numpy as np

import os.path as osp
from PIL import Image
import torch.utils.data as data
import torch

from .psp_data_augmentation import Compose, Scale, RandomRotation, RandomMirror, Resize, Normalize_Tensor

############################
## datsetはHPより以下のコードを使って事前にダウンロード ######

# フォルダ「data」が存在しない場合は作成する
#######　適宜修正してください  ##################
# data_dir = "/homes/ypark/code/dataset"
# ## abci用
# data_dir = "/groups/gaa50073/park-yuna/datasets"
# if not os.path.exists(data_dir):
#     os.mkdir(data_dir)


# # 公式のHOからVOC2012のデータセットをダウンロード
# # 時間がかかります（約15分）
# url = "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
# url = "http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar"

# target_path = os.path.join(data_dir, "VOCtrainval_11-May-2012.tar") 
# target_path = os.path.join(data_dir, "VOCtest_06-Nov-2007.tar") 

# if not os.path.exists(target_path):
#     urllib.request.urlretrieve(url, target_path)
    
#     tar = tarfile.TarFile(target_path)  # tarファイルを読み込み
#     tar.extractall(data_dir)  # tarを解凍
#     tar.close()  # tarファイルをクローズ
    
############################

# "/groups/gaa50073/park-yuna/datasets/VOCtest_06-Nov-2007.tar"
# "/groups/gaa50073/park-yuna/datasets/VOCtrainval_11-May-2012.tar"
# を解凍すると，
# "/groups/gaa50073/park-yuna/datasets/VOCdevkit/ - VOC2007 と - VOC2012 ができる"

# cfg.default.dataset_dir :  (SGE_LOCAL_DIR) + dataset/



# # どの画像がtrain, valにそれぞれ含まれるかを指定したtxtファイルから画像名のリストを取得
# def make_datapath_list(path_2012, path_2007):
#     """
#     学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する。

#     Parameters
#     ----------
#     rootpath : str
#         データフォルダへのパス

#     Returns
#     -------
#     ret : train_img_list, train_anno_list, val_img_list, val_anno_list
#         データへのパスを格納したリスト
#     """

#     # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成
#     imgpath_template = osp.join(path_2012, 'JPEGImages', '%s.jpg')
#     annopath_template = osp.join(path_2012, 'SegmentationClass', '%s.png')

#     test_imgpath_template = osp.join(path_2007, 'JPEGImages', '%s.jpg')
#     test_annopath_template = osp.join(path_2007, 'SegmentationClass', '%s.png')

#     # 訓練と検証、それぞれのファイルのID（ファイル名）を取得する
#     # ここにどのデータがtrainでどれがvalか書いてある
#     train_id_names = osp.join(path_2012 + 'split/train.txt')
#     val_id_names = osp.join(path_2012 + 'split/val.txt')
#     test_id_names = osp.join(path_2007 + 'ImageSets/Segmentation/test.txt')

#     # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成
#     train_img_list = list()
#     train_anno_list = list()

#     for line in open(train_id_names):
#         file_id = line.strip()  # 空白スペースと改行を除去
#         img_path = (imgpath_template % file_id)  # 画像のパス
#         anno_path = (annopath_template % file_id)  # アノテーションのパス
#         train_img_list.append(img_path)
#         train_anno_list.append(anno_path)

#     # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成
#     val_img_list = list()
#     val_anno_list = list()

#     for line in open(val_id_names):
#         file_id = line.strip()  # 空白スペースと改行を除去
#         img_path = (imgpath_template % file_id)  # 画像のパス
#         anno_path = (annopath_template % file_id)  # アノテーションのパス
#         val_img_list.append(img_path)
#         val_anno_list.append(anno_path)

#     # テストデータの画像ファイルとアノテーションファイルへのパスリストを作成
#     test_img_list = list()
#     test_anno_list = list()

#     for line in open(test_id_names):
#         file_id = line.strip()
#         img_path = (test_imgpath_template % file_id)
#         anno_path = (test_annopath_template % file_id)
#         test_img_list.append(img_path)
#         test_anno_list.append(anno_path)

#     return train_img_list, train_anno_list, val_img_list, val_anno_list, test_img_list, test_anno_list

def datapath_list(path_train, path_val, path_test):
    train_imgpath_template = osp.join(path_train, 'image','%s.jpg')
    val_imgpath_template = osp.join(path_val, 'image','%s.jpg')
    test_imgpath_template = osp.join(path_test, 'image', '%s.jpg')
    
    train_annopath_template = osp.join(path_train, 'label', '%s.png')
    val_annopath_template = osp.join(path_val, 'label', '%s.png')
    test_annopath_template = osp.join(path_test, 'label', '%s.png')
    
    train_id_names = osp.join(path_train + 'trainaug.txt')
    val_id_names = osp.join(path_val + 'val.txt')
    test_id_names = osp.join(path_test + 'test.txt') 
    
    # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成
    train_img_list = list()
    train_anno_list = list()

    for line in open(train_id_names):
        file_id = line.strip()  # 空白スペースと改行を除去
        img_path = (train_imgpath_template % file_id)  # 画像のパス
        anno_path = (train_annopath_template % file_id)  # アノテーションのパス
        train_img_list.append(img_path)
        train_anno_list.append(anno_path)

    # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成
    val_img_list = list()
    val_anno_list = list()

    for line in open(val_id_names):
        file_id = line.strip()  # 空白スペースと改行を除去
        img_path = (val_imgpath_template % file_id)  # 画像のパス
        anno_path = (val_annopath_template % file_id)  # アノテーションのパス
        val_img_list.append(img_path)
        val_anno_list.append(anno_path)

    # テストデータの画像ファイルとアノテーションファイルへのパスリストを作成
    test_img_list = list()
    test_anno_list = list()

    for line in open(test_id_names):
        file_id = line.strip()
        img_path = (test_imgpath_template % file_id)
        anno_path = (test_annopath_template % file_id)
        test_img_list.append(img_path)
        test_anno_list.append(anno_path)
        
    return train_img_list, train_anno_list, val_img_list, val_anno_list, test_img_list, test_anno_list


# TODO: testにも対応した仕様に変更する
class VOCDataset(data.Dataset):
    """
    VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。

    Attributes
    ----------
    img_list : リスト
        画像のパスを格納したリスト
    anno_list : リスト
        アノテーションへのパスを格納したリスト
    phase : 'train' or 'test'
        学習か訓練かを設定する。
    transform : object
        前処理クラスのインスタンス
    """

    def __init__(self, img_list, anno_list, phase, transform, img_size):
        self.img_list = img_list
        self.anno_list = anno_list
        self.phase = phase
        self.transform = transform
        self.img_size = img_size

    def __len__(self):
        '''画像の枚数を返す'''
        return len(self.img_list)

    def __getitem__(self, index):
        '''
        前処理をした画像のTensor形式のデータとアノテーションを取得
        '''
        img, anno_class_img = self.pull_item(index)
        sample = {'image': img, 'label': anno_class_img}

        # 3. データ拡張を実施
        if self.transform:
            sample = self.transform(sample)

        return sample



    def pull_item(self, index):
        '''画像のTensor形式のデータ、アノテーションを取得する'''

        # 1. 画像読み込み
        image_file_path = self.img_list[index]
        img = Image.open(image_file_path)   # [高さ][幅][色RGB]

        # 2. アノテーション画像読み込み
        anno_file_path = self.anno_list[index]
        anno_class_img = Image.open(anno_file_path)   # [高さ][幅]

        ### データごとにサイズが違うので均一のサイズにリサイズ
        resize_fn = Resize(self.img_size)
        img, anno_class_img = resize_fn(img, anno_class_img)

        # # 3. 前処理を実施
        # img, anno_class_img = self.transform(self.phase, img, anno_class_img)

        return img, anno_class_img
    



## どんなデータセットか確かめたいときに使用 ##

# rootpath = "/homes/ypark/code/dataset/VOCdevkit/VOC2012/"
# train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(
#     rootpath=rootpath)

# print(train_img_list[0])
# print(train_anno_list[0])

# # (RGB)の色の平均値と標準偏差
# color_mean = (0.485, 0.456, 0.406)
# color_std = (0.229, 0.224, 0.225)

# # データセット作成
# train_dataset = VOCDataset(train_img_list, train_anno_list, phase="train", transform=DataTransform(
#     input_size=475, color_mean=color_mean, color_std=color_std))

# val_dataset = VOCDataset(val_img_list, val_anno_list, phase="val", transform=DataTransform(
#     input_size=475, color_mean=color_mean, color_std=color_std))

# # データの取り出し例

# # jpeg画像には 縦，横，チャンネル数(RGB)のデータ torch.Size([3, 475, 475])
# # png画像 (正解ラベル) には 縦，横， torch.Size([475, 475]) ，1ピクセルにスカラー値
# # annoは0-20までのクラス，背景は0

# print(val_dataset.__getitem__(0)[0].shape)
# print(val_dataset.__getitem__(0)[1].shape)
# print(val_dataset.__getitem__(10)[1])
# for i in range(10):
#     anno_ex = val_dataset.__getitem__(i)[1]
#     max_value = torch.max(anno_ex)
#     min_value = torch.min(anno_ex)
#     print(f"最大値: {max_value}, 最小値: {min_value}")
#     print(f"unieque anno classes : {torch.unique(anno_ex)}")






##### PSPNetで使われているtransforms ** 参考までに #######
class DataTransform():
    """
    画像とアノテーションの前処理クラス。訓練時と検証時で異なる動作をする。
    画像のサイズをinput_size x input_sizeにする。
    訓練時はデータオーギュメンテーションする。


    Attributes
    ----------
    input_size : int
        リサイズ先の画像の大きさ。
    color_mean : (R, G, B)
        各色チャネルの平均値。
    color_std : (R, G, B)
        各色チャネルの標準偏差。
    """

    def __init__(self, input_size, color_mean, color_std):
        self.data_transform = {
            'train': Compose([
                Scale(scale=[0.5, 1.5]),  # 画像の拡大
                RandomRotation(angle=[-10, 10]),  # 回転
                RandomMirror(),  # ランダムミラー
                Resize(input_size),  # リサイズ(input_size)
                Normalize_Tensor(color_mean, color_std)  # 色情報の標準化とテンソル化
            ]),
            'val': Compose([
                Resize(input_size),  # リサイズ(input_size)
                Normalize_Tensor(color_mean, color_std)  # 色情報の標準化とテンソル化
            ])
        }

    def __call__(self, phase, img, anno_class_img):
        """
        Parameters
        ----------
        phase : 'train' or 'val'
            前処理のモードを指定。
        """
        return self.data_transform[phase](img, anno_class_img)

================
File: model/pspnet.py
================
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import model.resnet as models

class PPM(nn.Module):
    def __init__(self, in_dim, reduction_dim, bins):
        super(PPM, self).__init__()
        self.features = []
        for bin in bins:
            self.features.append(nn.Sequential(
                nn.AdaptiveAvgPool2d(bin),
                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),
                nn.BatchNorm2d(reduction_dim),
                nn.ReLU(inplace=True)
            ))
        self.features = nn.ModuleList(self.features)

    def forward(self, x):
        x_size = x.size()
        out = [x]
        for f in self.features:
            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))
        return torch.cat(out, 1)

class PSPNet(nn.Module):
    def __init__(self, layers=50, bins=(1, 2, 3, 6), dropout=0.1, classes=21, zoom_factor=8, use_ppm=True, criterion=nn.CrossEntropyLoss(ignore_index=255), pretrained=True):
        super(PSPNet, self).__init__()
        assert layers in [50, 101, 152]
        assert 2048 % len(bins) == 0
        assert classes > 1
        assert zoom_factor in [1, 2, 4, 8]
        self.zoom_factor = zoom_factor
        self.use_ppm = use_ppm
        self.criterion = criterion

        if layers == 50:
            resnet = models.resnet50(pretrained=False)
        elif layers == 101:
            resnet = models.resnet101(pretrained=False)
        else:
            resnet = models.resnet152(pretrained=False)

        if pretrained:
            model_path = os.path.join(os.path.dirname(__file__), 'initmodel', f'resnet{layers}_v2.pth')
            if os.path.isfile(model_path):
                resnet.load_state_dict(torch.load(model_path), strict=False)
                print(f"Loaded pretrained model from {model_path}")
            else:
                print(f"No pretrained model found at {model_path}, using ImageNet weights")
                if layers == 50:
                    resnet = models.resnet50(pretrained=True)
                elif layers == 101:
                    resnet = models.resnet101(pretrained=True)
                else:
                    resnet = models.resnet152(pretrained=True)

        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.conv2, resnet.bn2, resnet.relu, resnet.conv3, resnet.bn3, resnet.relu, resnet.maxpool)
        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4

        for n, m in self.layer3.named_modules():
            if 'conv2' in n:
                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)
            elif 'downsample.0' in n:
                m.stride = (1, 1)
        for n, m in self.layer4.named_modules():
            if 'conv2' in n:
                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)
            elif 'downsample.0' in n:
                m.stride = (1, 1)

        fea_dim = 2048
        if use_ppm:
            self.ppm = PPM(fea_dim, int(fea_dim/len(bins)), bins)
            fea_dim *= 2
        self.cls = nn.Sequential(
            nn.Conv2d(fea_dim, 512, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Dropout2d(p=dropout),
            nn.Conv2d(512, classes, kernel_size=1)
        )
        if self.training:
            self.aux = nn.Sequential(
                nn.Conv2d(1024, 256, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Dropout2d(p=dropout),
                nn.Conv2d(256, classes, kernel_size=1)
            )

    def forward(self, x, y=None):
        x_size = x.size()
        assert (x_size[2]-1) % 8 == 0 and (x_size[3]-1) % 8 == 0
        h = int((x_size[2] - 1) / 8 * self.zoom_factor + 1)
        w = int((x_size[3] - 1) / 8 * self.zoom_factor + 1)

        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x_tmp = self.layer3(x)
        x = self.layer4(x_tmp)
        if self.use_ppm:
            x = self.ppm(x)
        x = self.cls(x)
        if self.zoom_factor != 1:
            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)

        if self.training:
            aux = self.aux(x_tmp)
            if self.zoom_factor != 1:
                aux = F.interpolate(aux, size=(h, w), mode='bilinear', align_corners=True)
            main_loss = self.criterion(x, y)
            aux_loss = self.criterion(aux, y)
            return x.max(1)[1], main_loss, aux_loss
        else:
            return x

================
File: model/resnet.py
================
import torch
import torch.nn as nn
import math
import torch.utils.model_zoo as model_zoo


__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152']


model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=1000, deep_base=True):
        super(ResNet, self).__init__()
        self.deep_base = deep_base
        if not self.deep_base:
            self.inplanes = 64
            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
            self.bn1 = nn.BatchNorm2d(64)
        else:
            self.inplanes = 128
            self.conv1 = conv3x3(3, 64, stride=2)
            self.bn1 = nn.BatchNorm2d(64)
            self.conv2 = conv3x3(64, 64)
            self.bn2 = nn.BatchNorm2d(64)
            self.conv3 = conv3x3(64, 128)
            self.bn3 = nn.BatchNorm2d(128)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AvgPool2d(7, stride=1)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        if self.deep_base:
            x = self.relu(self.bn2(self.conv2(x)))
            x = self.relu(self.bn3(self.conv3(x)))
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x


def resnet18(pretrained=False, **kwargs):
    """Constructs a ResNet-18 model.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))
    return model


def resnet34(pretrained=False, **kwargs):
    """Constructs a ResNet-34 model.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))
    return model


def resnet50(pretrained=False, **kwargs):
    """Constructs a ResNet-50 model.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    if pretrained:
        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))
        model_path = os.path.join(os.path.dirname(__file__), 'initmodel', f'resnet50_v2.pth')
        model.load_state_dict(torch.load(model_path), strict=False)
    return model


def resnet101(pretrained=False, **kwargs):
    """Constructs a ResNet-101 model.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
    if pretrained:
        # model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))
        model_path = os.path.join(os.path.dirname(__file__), 'initmodel', f'resnet101_v2.pth')
        model.load_state_dict(torch.load(model_path), strict=False)
    return model


def resnet152(pretrained=False, **kwargs):
    """Constructs a ResNet-152 model.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)
    if pretrained:
        # model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))
        model_path = os.path.join(os.path.dirname(__file__), 'initmodel', f'resnet152_v2.pth')
        model.load_state_dict(torch.load(model_path), strict=False)
    return model

================
File: model/segnet.py
================
import torch
import torch.nn as nn
import torch.nn.functional as F





class SegNet(nn.Module):
    def __init__(self, input_channels, output_channels):
        super(SegNet, self).__init__()

        # Encoder layers

        self.encoder_0 = nn.Sequential(nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(64),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(64),
                                            nn.ReLU(inplace=True))

        self.encoder_1= nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(128),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(128),
                                            nn.ReLU(inplace=True))

        self.encoder_2 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(256),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(256),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(256),
                                            nn.ReLU(inplace=True))

        self.encoder_3 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512,out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True))

        self.encoder_4 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True))

        # Decoder layers

        self.decoder_4 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True))

        self.decoder_3 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(512),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(256),
                                            nn.ReLU(inplace=True))

        self.decoder_2 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(256),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(256),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(128),
                                            nn.ReLU(inplace=True))

        self.decoder_1 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(128),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(64),
                                            nn.ReLU(inplace=True))

        self.decoder_0 = nn.Sequential(nn.Conv2d(in_channels=64,out_channels=64, kernel_size=3, padding=1),
                                            nn.BatchNorm2d(64),
                                            nn.ReLU(inplace=True),
                                            nn.Conv2d(in_channels=64, out_channels=output_channels, kernel_size=1))

        self._init_weight()

    def forward(self, x):
        """
        Forward pass `input_img` through the network
        """

        # Encoder

        # Encoder Stage - 1
        dim_0 = x.size()
        x = self.encoder_0(x)
        x, indices_0 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)

        # Encoder Stage - 2
        dim_1 = x.size()
        x = self.encoder_1(x)
        x, indices_1 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)

        # Encoder Stage - 3
        dim_2 = x.size()
        x = self.encoder_2(x)
        x, indices_2 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)

        # Encoder Stage - 4
        dim_3 = x.size()
        x = self.encoder_3(x)
        x, indices_3 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)

        # Encoder Stage - 5
        dim_4 = x.size()
        x = self.encoder_4(x)
        x, indices_4 = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)

        # Decoder

        #dim_d = x.size()

        # Decoder Stage - 5
        x = F.max_unpool2d(x, indices_4, kernel_size=2, stride=2, output_size=dim_4)
        x = self.decoder_4(x)
        #dim_4d = x.size()

        # Decoder Stage - 4
        x = F.max_unpool2d(x, indices_3, kernel_size=2, stride=2, output_size=dim_3)
        x = self.decoder_3(x)
        #dim_3d = x.size()

        # Decoder Stage - 3
        x = F.max_unpool2d(x, indices_2, kernel_size=2, stride=2, output_size=dim_2)
        x = self.decoder_2(x)
        #dim_2d = x.size()

        # Decoder Stage - 2
        x = F.max_unpool2d(x, indices_1, kernel_size=2, stride=2, output_size=dim_1)
        x = self.decoder_1(x)

        # Decoder Stage - 1
        x = F.max_unpool2d(x, indices_0, kernel_size=2, stride=2, output_size=dim_0)
        x = self.decoder_0(x)
        
        return x

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

================
File: debug/check.py
================
from PIL import Image
import os

"""
This code is used for searching and deleting the broken images and their corresponding files.
"""

def check_and_delete_images(directory):
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                file_path = os.path.join(root, file)
                try:
                    img = Image.open(file_path)
                    img.verify()  # Confirm if the image is valid
                except Exception as e:
                    print(f"破損した画像ファイル {file_path}: {e}")
                    # Extract the file number (assuming the format is consistent and number can be extracted)
                    file_number = os.path.splitext(file)[0]
                    # Find and delete corresponding files in sibling directories
                    delete_corresponding_files(root, file_number)

def delete_corresponding_files(root, file_number):
    """
    This function is defined as follows.
    1. confirm the current dir that the broken image belongs to
    2. go to the parent dir 
    3. go into the every sub dir beloning to the parent dir
    4. delete the image if the number is as same as the broken image
    """
    base_dir = os.path.dirname(root)
    for subdir in os.listdir(base_dir):
        subdir_path = os.path.join(base_dir, subdir)
        if os.path.isdir(subdir_path):
            for file in os.listdir(subdir_path):
                if file.startswith(file_number) and file.endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                    file_to_delete = os.path.join(subdir_path, file)
                    os.remove(file_to_delete)
                    print(f"削除したファイル: {file_to_delete}")

check_and_delete_images('/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/CityScapes')

================
File: debug/check_label.py
================
from PIL import Image
import numpy as np

# サンプル画像のパス
sample_path = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/train_aug/label/2008_005869.png"

with Image.open(sample_path) as img:
    label_array = np.array(img)
    print(f"Shape: {label_array.shape}")
    print(f"Data type: {label_array.dtype}")
    print(f"Unique values: {np.unique(label_array)}")
    print(f"Min value: {np.min(label_array)}")
    print(f"Max value: {np.max(label_array)}")

================
File: debug/find_label.py
================
import os
from PIL import Image
import numpy as np

label_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/train_aug/label"
trainaug_file = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/train_aug/trainaug.txt"

with open(trainaug_file, 'r') as f:
    image_names = [line.strip() for line in f.readlines()]

unannotated_images = []

for image_name in image_names:
    label_path = os.path.join(label_dir, f"{image_name}.png")
    
    if not os.path.exists(label_path):
        unannotated_images.append(image_name)
        continue
    
    try:
        with Image.open(label_path) as img:
            label_array = np.array(img)
            if len(np.unique(label_array)) == 1 and np.unique(label_array)[0] == 0:
                unannotated_images.append(image_name)
    except Exception as e:
        print(f"Error processing {image_name}: {str(e)}")
        unannotated_images.append(image_name)

print(f"Total images in trainaug.txt: {len(image_names)}")
print(f"Number of effectively unannotated images: {len(unannotated_images)}")
print("Effectively unannotated images:")
for img in unannotated_images:
    print(img)

with open("effectively_unannotated_images.txt", "w") as f:
    for img in unannotated_images:
        f.write(f"{img}\n")

print("Results saved to effectively_unannotated_images.txt")

================
File: make_dataset/check.py
================
def read_file(file_path):
    with open(file_path, 'r') as file:
        return set(file.read().splitlines())

# ファイルパスを定義
sbd_trainval_path = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc/VOCdevkit/VOC2007/ImageSets/Segmentation/test.txt"
voc_test_path = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/test/test.txt"

# ファイルの内容を読み込む
sbd_trainval = read_file(sbd_trainval_path)
voc_test = read_file(voc_test_path)

# SBD trainvalセットがVOC testセットを完全に内包しているか確認
is_subset = voc_test.issubset(sbd_trainval)

print(f"SBD trainval set size: {len(sbd_trainval)}")
print(f"VOC test set size: {len(voc_test)}")

if is_subset:
    print("VOC test set is completely included in SBD trainval set.")
else:
    print("VOC test set is NOT completely included in SBD trainval set.")
    
    # 内包されていない要素を表示
    not_included = voc_test - sbd_trainval
    print(f"Number of elements in VOC test set not included in SBD trainval: {len(not_included)}")
    print("Sample of not included elements (up to 10):")
    for item in list(not_included)[:10]:
        print(item)

# 重複している要素の数を表示
intersection = voc_test.intersection(sbd_trainval)
print(f"\nNumber of elements common to both sets: {len(intersection)}")
print(f"Percentage of VOC test set included in SBD trainval: {len(intersection) / len(voc_test) * 100:.2f}%")

================
File: make_dataset/convert.py
================
import os
import numpy as np
from scipy.io import loadmat
from PIL import Image

def convert_mat_to_png(input_dir, output_dir):
    # 出力ディレクトリが存在しない場合は作成
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 入力ディレクトリ内の全.matファイルを処理
    for filename in os.listdir(input_dir):
        if filename.endswith('.mat'):
            mat_path = os.path.join(input_dir, filename)
            png_path = os.path.join(output_dir, filename.replace('.mat', '.png'))

            # .matファイルを読み込む
            mat_contents = loadmat(mat_path)
            
            # 'GTcls' キーの下の 'Segmentation' フィールドからデータを取得
            segmentation_data = mat_contents['GTcls']['Segmentation'][0][0]

            # NumPy配列をPIL Imageに変換
            img = Image.fromarray(segmentation_data.astype(np.uint8))

            # PNG形式で保存
            img.save(png_path)

            print(f"Converted {filename} to PNG")

    print("Conversion completed.")

# 入力と出力のディレクトリパスを設定
input_directory = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/train_aug/cls"
output_directory = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/train_aug/label"

# 変換を実行
convert_mat_to_png(input_directory, output_directory)

================
File: make_dataset/create_sbd.py
================
import os
import shutil

def read_file(file_path):
    with open(file_path, 'r') as file:
        return set(file.read().splitlines())

def write_file(file_path, content):
    with open(file_path, 'w') as file:
        file.write('\n'.join(content))

# Define paths
base_path = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/vocsbd"
voc_path = os.path.join(base_path, "VOC2012")
sbd_path = os.path.join(base_path, "SBD")
output_path = os.path.join(base_path, "trainaug.txt")

# Read VOC2012 train set
voc_train_path = os.path.join(voc_path, "train.txt")
voc_train = read_file(voc_train_path)

# Read SBD train and val sets
sbd_train_path = os.path.join(sbd_path, "train.txt")
sbd_val_path = os.path.join(sbd_path, "val.txt")
sbd_train = read_file(sbd_train_path)
sbd_val = read_file(sbd_val_path)

# Combine all sets
trainaug = voc_train.union(sbd_train).union(sbd_val)

print(f"VOC2012 train set size: {len(voc_train)}")
print(f"SBD train set size: {len(sbd_train)}")
print(f"SBD val set size: {len(sbd_val)}")
print(f"Combined trainaug set size: {len(trainaug)}")

if len(trainaug) != 10582:
    print(f"Warning: trainaug set size ({len(trainaug)}) is not 10,582 as expected.")
else:
    print("Successfully created trainaug set with 10,582 images.")

# Sort and write the trainaug set
sorted_trainaug = sorted(trainaug)
write_file(output_path, sorted_trainaug)
print(f"Created {output_path} with {len(sorted_trainaug)} entries.")

# Copy SegmentationClassAug to VOC2012 folder
seg_aug_src = os.path.join(sbd_path, "SegmentationClassAug")
seg_aug_dst = os.path.join(voc_path, "SegmentationClassAug")
if not os.path.exists(seg_aug_dst):
    shutil.copytree(seg_aug_src, seg_aug_dst)
    print(f"Copied SegmentationClassAug from {seg_aug_src} to {seg_aug_dst}")
else:
    print("SegmentationClassAug already exists in VOC2012 folder.")

# Output some sample entries for verification
print("\nSample entries from the trainaug set:")
for entry in sorted_trainaug[:10]:
    print(entry)

================
File: make_dataset/list.py
================
import os
import shutil

def copy_files(source_dir, target_dir, file_list, extension):
    # ターゲットディレクトリが存在しない場合は作成
    os.makedirs(target_dir, exist_ok=True)
    
    # ファイルリストを読み込む
    with open(file_list, 'r') as f:
        files = f.read().splitlines()
    
    copied_count = 0
    for file in files:
        source_file = os.path.join(source_dir, file + extension)
        target_file = os.path.join(target_dir, file + extension)
        
        if os.path.exists(source_file):
            shutil.copy2(source_file, target_file)
            copied_count += 1
    
    return copied_count, len(files)

# パスの設定
voc_image_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc/VOCdevkit/VOC2012/JPEGImages"
voc_label_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc/VOCdevkit/VOC2012/SegmentationClass"
val_list = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/val/val.txt"
target_image_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/val/image"
target_label_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/val/label"

# 画像のコピー
copied_images, total_images = copy_files(voc_image_dir, target_image_dir, val_list, ".jpg")
print(f"Copied {copied_images} out of {total_images} images.")
if copied_images == total_images:
    print("All images in val.txt have been successfully copied.")
else:
    print(f"Warning: {total_images - copied_images} images were not found or couldn't be copied.")

# ラベル（セグメンテーション）画像のコピー
copied_labels, total_labels = copy_files(voc_label_dir, target_label_dir, val_list, ".png")
print(f"Copied {copied_labels} out of {total_labels} label images.")
if copied_labels == total_labels:
    print("All label images in val.txt have been successfully copied.")
else:
    print(f"Warning: {total_labels - copied_labels} label images were not found or couldn't be copied.")

================
File: loss/entropy.py
================
import torch.nn as nn



class CrossEntropyLoss(nn.Module):
    def __init__(self, cfg):
        super(CrossEntropyLoss, self).__init__()
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=cfg.dataset.ignore_label)

    def forward(self, pred, target):
        return self.ce_loss(pred, target)
    

# 損失関数の設定
class PSPLoss(nn.Module):
    """PSPNetの損失関数のクラスです。"""

    def __init__(self, cfg):
        super(PSPLoss, self).__init__()
        self.aux_weight = cfg.optimizer.loss.aux_weight  # aux_lossの重み
        self.criterion = CrossEntropyLoss(cfg)

    def forward(self, outputs, targets):
        """
        損失関数の計算。

        Parameters
        ----------
        outputs : PSPNetの出力(tuple)
            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。

        targets : [num_batch, 475, 475]
            正解のアノテーション情報

        Returns
        -------
        loss : テンソル
            損失の値
        """

        loss = self.criterion(outputs[0], targets)
        loss_aux = self.criterion(outputs[1], targets)

        return loss+self.aux_weight*loss_aux

================
File: utils/common.py
================
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from pathlib import Path 
import shutil
import random

import torch


# usible cudaの取得
# return : device
# -> model.to(device), data.to(devide) で使用
def setup_device(cfg):
    if torch.cuda.is_available():
        device = torch.device(f"cuda:{cfg.default.device_id}")
        
        if not cfg.default.deterministic:
            torch.backends.cudnn.benchmark = True
    else:
        device = "cpu"
    print("CUDA is available:", torch.cuda.is_available())
    print(f"using device: {device}")
    return device


# 全体のseed値の固定
# pytorchやnumpy変数など全てのseed値が固定される
# プログラムの初めで呼び出してseedを固定する必要
def fixed_r_seed(cfg):
    random.seed(cfg.default.seed)
    np.random.seed(cfg.default.seed)
    torch.manual_seed(cfg.default.seed)
    torch.cuda.manual_seed(cfg.default.seed)
    torch.backends.cudnn.deterministic = True
    torch.use_deterministic_algorithms = True


# start = time.time()
# end = time.time()
# interval = start - end
# intervalを入れると秒を時間，分，秒に直して辞書型で返す
def get_time(interval):
    time = {"time" : "{}h {}m {}s".format(
            int(interval / 3600), 
            int((interval % 3600) / 60), 
            int((interval % 3600) % 60))}
    return time


# plot loss and acc curve
def plot_log(cfg, data):
    epochs = data['epoch']
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9), dpi=80)
    
    # Loss plot
    ax1.plot(epochs, data["train_loss"], label='Train Loss', alpha=0.8, linewidth=2, color='tab:blue')
    ax1.plot(epochs, data["val_loss"], label='Val Loss', alpha=0.8, linewidth=2, color='tab:orange')
    ax1.set_title('Loss', fontsize=30)
    ax1.set_xlabel('Epochs', fontsize=25)
    ax1.set_ylabel('Loss', fontsize=25)
    ax1.legend(fontsize=20, loc='upper right')
    ax1.tick_params(labelsize=20)
    ax1.grid(True)

    # mIoU and Accuracy plot
    ax2_1 = ax2
    ax2_2 = ax2.twinx()
    
    ax2_1.plot(epochs, data["val_mIoU"], label='mIoU', alpha=0.8, linewidth=2, color='tab:green')
    ax2_2.plot(epochs, data["val_acc"], label='Accuracy', alpha=0.8, linewidth=2, color='tab:red')

    ax2.set_title('Validation Metrics', fontsize=30)
    ax2.set_xlabel('Epochs', fontsize=25)
    ax2_1.set_ylabel('mIoU', fontsize=25, color='tab:green')
    ax2_2.set_ylabel('Accuracy', fontsize=25, color='tab:red')

    ax2_1.tick_params(labelsize=20, axis='y', colors='tab:green')
    ax2_2.tick_params(labelsize=20, axis='y', colors='tab:red')

    lines1, labels1 = ax2_1.get_legend_handles_labels()
    lines2, labels2 = ax2_2.get_legend_handles_labels()
    ax2.legend(lines1 + lines2, labels1 + labels2, fontsize=20, loc='lower right')

    fig.suptitle(f"{cfg.out_dir}", fontsize=16)
    plt.tight_layout()
    plt.savefig(cfg.out_dir + "graph.png")
    plt.close()


# show sample 12 imgs
def show_img(cfg, dataloader):
    for batched in dataloader:
        images = batched["image"]
        labels = batched["label"]
        break
    
    fig, axes = plt.subplots(3, 4, figsize=(12, 9))
    for i in range(12):
        ax = axes[i // 4, i % 4]
        img = np.transpose(images[i].numpy(), (1, 2, 0))  
        ax.imshow(img)
        ax.set_title(f"Label: {labels[i]}")
        ax.axis('off')

    plt.savefig(cfg.out_dir + "img.png")
    plt.close()



# Afinity-Weighted RA使用時に使う
# plot the num of selected method (read from csv file)
def plot_selected(cfg):
    file_path = cfg.out_dir + f"selected_method_{cfg.augment.ra.weight}.csv"

    df = pd.read_csv(file_path)

    fig, ax = plt.subplots(figsize=(16, 9))  

    interval = cfg.learn.n_epoch*10
    if type(cfg.save.interval) == int:
        interval = cfg.save.interval
    for method in df.columns:
        values = df.loc[:, method]
        ax.plot(range(1, len(values)+1, interval), values.iloc[::interval], label=method, marker='o', markersize=2)

    ax.set_xlabel('Iteration', fontsize=25)
    ax.set_ylabel('Count', fontsize=25)
    ax.set_title('Selected method', fontsize=30)
    fig.suptitle(f"{cfg.out_dir}")
    ax.tick_params(labelsize=25)
    ax.legend()
    plt.grid(True)
    plt.savefig(cfg.out_dir + "selected.png")
    plt.close()


# abci使用時に使う
def copy_from_sge(cfg, target_dir_name):
    sge_dir = str(Path(cfg.default.dataset_dir).parent)
    files = [f for f in os.listdir(sge_dir) if os.path.isfile(os.path.join(sge_dir, f))]
    for file in files:
        if target_dir_name in file:
            source_path = os.path.join(sge_dir, file)
            destination_path = os.path.join(cfg.out_dir, file)
            shutil.copy2(source_path, destination_path)


# abci使用時に使う
def copy_to_sge(cfg, target_path):
    sge_dir = str(Path(cfg.default.dataset_dir).parent)
    shutil.copytree(target_path, sge_dir)


# 学習済みモデルの重みを保存
# if BEST == False:
#   学習途中のモデル重みを保存するときに使う
#   weight/latest_epochs.pth に保存される
# elif BEST == True:
#   val accが最良のモデル重みを保存するときに使う
#   weight/best.pth に保存される
def save_learner(cfg, model, device, BEST=False):
    weight_dir_path = cfg.out_dir + "weights/"
    os.makedirs(weight_dir_path, exist_ok=True)
    if BEST:
        save_file_path = weight_dir_path + "best.pth"
    else:
        save_file_path = weight_dir_path + "latest_epochs.pth"

    torch.save(
        model.to("cpu").state_dict(),
        save_file_path,
    )
    model.to(device)


# 毎エポックのモデル重みを保存する時に使用，基本的には使わない
# def save_all_learner(cfg, model, device, epoch):
#     weight_dir_path = cfg.out_dir + "weights/"
#     os.makedirs(weight_dir_path, exist_ok=True)
#     save_file_path = weight_dir_path + f"{epoch}.pth"

#     torch.save(
#         model.to("cpu").state_dict(),
#         save_file_path,
#     )
#     model.to(device)


def lr_step(cfg, scheduler, epoch):
        if cfg.optimizer.scheduler.name == "warmup":
            scheduler.step(epoch)

        else:
            scheduler.step()

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def intersectionAndUnionGPU(output, target, K, ignore_index=255):
    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.
    assert (output.dim() in [1, 2, 3])
    assert output.shape == target.shape
    output = output.view(-1)
    target = target.view(-1)
    output[target == ignore_index] = ignore_index
    intersection = output[output == target]
    area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)
    area_output = torch.histc(output, bins=K, min=0, max=K-1)
    area_target = torch.histc(target, bins=K, min=0, max=K-1)
    area_union = area_output + area_target - area_intersection
    return area_intersection, area_union, area_target

================
File: utils/lr_scheduler.py
================
from torch.optim.lr_scheduler import _LRScheduler

class PolyLR(_LRScheduler):
    """
    This class implements the polynomial learning rate schedule.
    """
    def __init__(self, optimizer, T_max, eta_min=0, power=0.9, last_epoch=-1):
        self.T_max = T_max
        self.eta_min = eta_min
        self.power = power
        super(PolyLR, self).__init__(optimizer, last_epoch)

    def get_lr(self):
        return [self.eta_min + (base_lr - self.eta_min) *
                (1 - self.last_epoch / self.T_max) ** self.power
                for base_lr in self.base_lrs]

================
File: utils/suggest.py
================
import random
import numpy as np

import torch
import torch.nn as nn
from torchvision.models import vit_b_16, vgg16_bn

from model.segnet import SegNet
from model.pspnet import PSPNet
from loss.entropy import CrossEntropyLoss, PSPLoss
from utils.lr_scheduler import PolyLR # polynomialスケジューラーのアルゴリズムが記載されている



# configをもとに使用モデルを識別，呼び出す
# 後にsegnet以外も使いたい時があるかも
def suggest_network(cfg):
    if cfg.network.name == "segnet":
        print("Selected network is a SEGNET!")
        model = SegNet(input_channels=3, output_channels=cfg.dataset.n_class)
    elif cfg.network.name == "pspnet":
        print("Selected network is a PSPNET!")
        model = PSPNet(
            layers=cfg.network.resenet_layers,
            bins=(1, 2, 3, 6),
            dropout=cfg.network.dropout_rate,
            classes=cfg.dataset.n_class,
            zoom_factor=cfg.network.zoom_factor,
            use_ppm=cfg.network.use_ppm,
            pretrained=cfg.network.pretrained  # ここを True に設定
        )
    return model


# configをもとに最適化手法を設定
# 学習率などのハイパラもconfigを参照
def suggest_optimizer(cfg, model):
    if cfg.optimizer.name == "SGD":
        optimizer = torch.optim.SGD(
            params=model.parameters(), 
            lr=cfg.optimizer.hp.lr, 
            momentum=cfg.optimizer.hp.momentum, 
            weight_decay=cfg.optimizer.hp.weight_decay, 
            nesterov=True,
        )
    elif cfg.optimizer.name == "AdamW":
        optimizer = torch.optim.AdamW(
            params=model.parameters(),
            lr = cfg.optimizer.hp.lr,
            weight_decay=cfg.optimizer.hp.weight_decay,
        )
    elif cfg.optimizer.name == "Adam":
        optimizer = torch.optim.Adam(
            params=model.parameters(),
            lr = cfg.optimizer.hp.lr,
            weight_decay=cfg.optimizer.hp.weight_decay,
        )
        
    else:
        raise ValueError(f"Invalid optimizer ... {cfg.optimizer.name}, Select from < SGE, Adam >.")
    
    return optimizer


# 学習率の設定，基本的にはcosine decayで良いのでは.. -> polyが最適！！
def suggest_scheduler(cfg, optimizer):
    if cfg.optimizer.scheduler.name == "fix":
        scheduler = torch.optim.lr_scheduler.MultiStepLR(
            optimizer,
            milestones=[],
            gamma=1.0,
        )

    elif cfg.optimizer.scheduler.name == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=cfg.learn.n_epoch,
            eta_min=cfg.optimizer.hp.lr_min,
        )

    elif cfg.optimizer.scheduler.name == "step":
        scheduler = torch.optim.lr_scheduler.MultiStepLR(
            optimizer,
            milestones=cfg.optimizer.scheduler.step,
            gamma=0.1,
        )

    elif cfg.optimizer.scheduler.name == "poly":
        scheduler = PolyLR(
            optimizer,
            T_max=cfg.learn.n_epoch,
            eta_min=cfg.optimizer.hp.lr_min,
            power=cfg.optimizer.scheduler.power
        )
        
   
    else:
        raise ValueError(f"Invalid Lr Scheduler ... {cfg.optimizer.scheduler.name}, select from < cosine, step >")
    
    return scheduler


# loss/entropy.pyの中に定義されているCrossEntropyLossクラスを呼び出す
def suggest_loss_func(cfg):
    if cfg.optimizer.loss.name == "ce":
        if cfg.network.name == "pspnet":
            criterion = PSPLoss(cfg)
        else:
            criterion = CrossEntropyLoss(cfg)
    else: 
        raise ValueError("incorrect loss name ... available : ce")
    
    return criterion

================
File: scripts/abci/run.bash
================
#!/bin/bash
#$ -l rt_G.small=1
#$ -l h_rt=10:00:00
#$ -j y
#$ -o /groups/gaa50073/kohata-yuto/segmentation/src/trash/
#$ -cwd

source /etc/profile.d/modules.sh
module load  python/3.11/3.11.9
source ~/.bashrc
conda activate seg-env

WORKDIR="/groups/gaa50073/kohata-yuto/segmentation/src/"
echo "ok"
echo $WORKDIR

mkdir -p $SGE_LOCALDIR/datasets
cp -v "/groups/gaa50073/kohata-yuto/voc_datasets.tar.gz" $SGE_LOCALDIR/datasets/
cd $SGE_LOCALDIR/datasets
tar -I pigz -xf "voc_datasets.tar.gz"
ls

cd $WORKDIR

# case $SGE_TASK_ID in
#     1) seed=502;;
#     2) seed=503;;
#     *) echo "Invalid task ID"; exit 1;;
# esac

seed=1

python main.py voc_abci \
    default.device_id=0 \
    default.dataset_dir="$SGE_LOCALDIR/datasets/" \
    default.seed=$seed \
    augment.name=["ra"] \
    augment.ra.weight="single" \
    augment.ra.single="ShearX"

# 実行するとき
# $SGE_TASK_IDを設定するとき，
# qsub -g gaa50073 -t 1-2:1 /groups/gaa50073/park-yuna/cont/src/scripts/s0725/aff_inv.sh
# $SGE_TASK_IDを設定しないとき，
# qsub -g gaa50073 /groups/gaa50073/park-yuna/cont/src/scripts/s0725/aff_inv.sh

================
File: scripts/abci/test.sh
================
#!/bin/bash
#$ -l rt_G.small=1
#$ -l h_rt=6:00:00
#$ -j y
#$ -o /groups/gaa50073/park-yuna/segmentation/comi/out0714/voc/
#$ -cwd

source /etc/profile.d/modules.sh
module load  python/3.11/3.11.9
source ~/.bashrc
conda activate new

WORKDIR="/groups/gaa50073/park-yuna/segmentation/src/"
echo "ok"

mkdir -p $SGE_LOCALDIR/datasets
cp -v "/groups/gaa50073/park-yuna/datasets/VOCtest_06-Nov-2007.tar" $SGE_LOCALDIR/datasets/
cp -v "/groups/gaa50073/park-yuna/datasets/VOCtrainval_11-May-2012.tar" $SGE_LOCALDIR/datasets/
cd $SGE_LOCALDIR/datasets
ls
# tar -I pigz -xf "VOCtest_06-Nov-2007.tar"
# tar -I pigz -xf "VOCtrainval_11-May-2012.tar"
tar -xf "VOCtest_06-Nov-2007.tar"
tar -xf "VOCtrainval_11-May-2012.tar"
ls
ls VOCdevkit

# Clone the repository and checkout the specified branch
GIT_REPO_DIR="/groups/gaa50073/park-yuna/segmentation"
cd $GIT_REPO_DIR
git fetch origin
git checkout feat-voc-test

cd $WORKDIR

# case $SGE_TASK_ID in
#     1) seed=502;;
#     2) seed=503;;
#     3) seed=504;;
#     *) echo "Invalid task ID"; exit 1;;
# esac
seed=1

python main.py voc_abci \
    default.dataset_dir="$SGE_LOCALDIR/datasets/" \
    default.seed=$seed \
    augment.name=["nan"]

================
File: scripts/cotton/run_cotton.sh
================
#!/bin/bash
#$ -j 
#$ -o /homes/ykohata/code/devml/homes/ypark/code/seg/src/trash/
#$ -cwd

source /etc/profile.d/modules.sh
module load  python/3.11/3.11.2
source ~/.bashrc
conda activate new-ra

WORKDIR=/homes/ykohata/code/devml/homes/ypark/code/seg
echo "ok"

cd $WORKDIR/src

seed=501
python main.py voc \
    default.dataset_dir="/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug/" \
    default.seed=$seed \
    learn.n_epoch=2 \
    augment.name=["nan"] \
    augment.ra.weight="single" \
    augment.ra.single="Cutout" \
    && python notify.py 0 || python notify.py 1

================
File: scripts/cotton/run_park.sh
================
#!/bin/bash
#$ -j 
#$ -o $(dirname $(dirname $(dirname $0)))/trash
#$ -cwd

source /etc/profile.d/modules.sh
module load  python/3.11/3.11.2
source ~/.bashrc
conda activate new-ra

WORKDIR=$(dirname $(dirname $(dirname $0)))
echo "ok"
echo $WORKDIR

cd $WORKDIR


seed=101
python main.py test \
    default.dataset_dir="/homes/ypark/code/dataset/" \
    augment.name=["cutout"]

================
File: scripts/cotton/run_test.sh
================
#!/bin/bash
#$ -j 
#$ -o /homes/ykohata/code/devml/homes/ypark/code/seg/trash
#$ -cwd

source /etc/profile.d/modules.sh
module load python/3.11/3.11.9
source ~/.bashrc
conda activate new

WORKDIR=/homes/ykohata/code/devml/homes/ypark/code/seg

echo "ok"

cd $WORKDIR/src

$seed=101
python test.py voc \
    default.dataset_dir="/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/voc_aug" \
    default.seed=$seed \
    learn.n_epoch=250 \
    augment.name=["ra"] \
    augment.ra.weight="single" \
    augment.ra.single="ShearX" \
    test.best_model="" \
    result_dir=""

================
File: scripts/cotton/voc_park.sh
================
#!/bin/bash
#$ -j 
#$ -o $(dirname $(dirname $(dirname $0)))/trash
#$ -cwd

source /etc/profile.d/modules.sh
module load  python/3.11/3.11.2
source ~/.bashrc
conda activate new

WORKDIR=$(dirname $(dirname $(dirname $0)))
echo "ok"
echo $WORKDIR

cd $WORKDIR


seed=101
python main.py voc \
    default.dataset_dir="/homes/ypark/code/dataset/" \
    augment.name=["nan"] \
    learn.n_epoch=2

================
File: make_dataset/download/README.md
================
# downloadディレクトリの中身について
**各ファイル内の絶対パスは適宜書き換えてください**

## 構成（シェルスクリプトは除く）
- download.py
- merge.py
- conc.py

## download.pyについて
### 概要
このファイルは `root_dir`配下に以下のデータセットをインターネットから拾ってきて、任意のディレクトリ構造にするものである。
- PASCAL VOC2012（train , validation用）
- PASCAL VOC2007（test用）
- Semantic Boundary Dataset（train, validation用でデータの水増し）

### 詳細
PASCAL VOCは公式ホームページから取得して解凍するようになっている。
一方でSBDは公式ホームページのリンクが削除されているのでPytorchの `torchvision.datasets.SBDataset`　を用いて取得するようにしている。
また `tqdm` を使用してダウンロードの状況が逐次的にわかるようにしている。
データセットがダウンロードできたらSBDとPASCAL VOC2012を統合してデータ数を水増ししている。これは先行研究を参考にしている。

## merge.pyについて
###　概要
SBDはラベルの形式が `.mat`拡張子なのに対して統合先のPASCAL VOC2012は `.png`拡張子となっている。そこでこれを `.png`に統一するためにこのコードを動かす。

### 詳細
- ファイル形式の違い:

VOCのラベルは.png形式
SBDのラベルは.mat形式


- SBDデータの構造:

clsディレクトリには、カテゴリ別のセグメンテーションとバウンダリが.mat形式で保存されています。
各.matファイルにはGTclsという構造体が含まれており、セグメンテーション情報が格納されています。


- 正しい統合方法:

SBDの.matファイルをそのままVOC2012のディレクトリ構造にコピーするのではなく、.matファイルから情報を抽出し、VOC2012と互換性のある.png形式に変換する必要があります。


- トレーニングとバリデーションの分割:

SBDのtrain.txtとval.txtを使用して、トレーニングセットとバリデーションセットを決定します。



- 修正したアプローチ:

SBDの.matファイルを読み込み、セグメンテーション情報を抽出します。
抽出したセグメンテーション情報を.png形式で保存します。
これらの新しく生成された.pngファイルを、VOC2012の構造内の新しいディレクトリ（例：SegmentationClassAug）に保存します。
SBDのtrain.txtとval.txtをVOC2012のディレクトリ構造内にコピーします。

## conc.pyについて
### 概要
SBDとVOCのそれぞれの画像とラベルとtxtファイルの統合

### 詳細
1. 画像のディレクトリの統合
2. ラベルのディレクトリの統合
3. splitファイルの統合
4. 画像とラベルが一対一対応かのテストおよびデバッグのコード

================
File: make_dataset/download/conc.py
================
import os
import shutil
from tqdm import tqdm
import random

# 基本ディレクトリの設定
base_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/vocsbd"
voc_dir = os.path.join(base_dir, "VOC2012")
vocdevkit_dir = os.path.join(voc_dir, "VOCdevkit", "VOC2012")
vocsbd_dir = os.path.join(base_dir, "VOCSBD")

# 新しいディレクトリの作成
os.makedirs(os.path.join(vocsbd_dir, "JPEGImages"), exist_ok=True)
os.makedirs(os.path.join(vocsbd_dir, "SegmentationClass"), exist_ok=True)
os.makedirs(os.path.join(vocsbd_dir, "split"), exist_ok=True)

# 1. JPEGImagesの統合
print("Merging JPEGImages...")
sbd_jpg_dir = os.path.join(voc_dir, "JPEGImages")
voc_jpg_dir = os.path.join(vocdevkit_dir, "JPEGImages")
merged_jpg_dir = os.path.join(vocsbd_dir, "JPEGImages")

for jpg_dir in [sbd_jpg_dir, voc_jpg_dir]:
    for img in tqdm(os.listdir(jpg_dir)):
        if img.endswith('.jpg'):
            src = os.path.join(jpg_dir, img)
            dst = os.path.join(merged_jpg_dir, img)
            if not os.path.exists(dst):
                shutil.copy2(src, dst)

# 2. SegmentationClassの統合
print("Merging SegmentationClass...")
sbd_seg_dir = os.path.join(voc_dir, "SegmentationClassAug")
voc_seg_dir = os.path.join(vocdevkit_dir, "SegmentationClass")
merged_seg_dir = os.path.join(vocsbd_dir, "SegmentationClass")

for seg_dir in [sbd_seg_dir, voc_seg_dir]:
    for img in tqdm(os.listdir(seg_dir)):
        if img.endswith('.png'):
            src = os.path.join(seg_dir, img)
            dst = os.path.join(merged_seg_dir, img)
            if not os.path.exists(dst):
                shutil.copy2(src, dst)

# 3. split ファイルの統合
print("Merging split files...")
sbd_train = os.path.join(voc_dir, "train.txt")
sbd_val = os.path.join(voc_dir, "val.txt")
voc_train = os.path.join(vocdevkit_dir, "ImageSets", "Segmentation", "train.txt")
voc_val = os.path.join(vocdevkit_dir, "ImageSets", "Segmentation", "val.txt")

merged_train = os.path.join(vocsbd_dir, "split", "train.txt")
merged_val = os.path.join(vocsbd_dir, "split", "val.txt")

def merge_txt_files(file1, file2, output_file):
    with open(file1, 'r') as f1, open(file2, 'r') as f2, open(output_file, 'w') as out:
        lines = set(f1.read().splitlines() + f2.read().splitlines())
        out.write('\n'.join(sorted(lines)))

merge_txt_files(sbd_train, voc_train, merged_train)
merge_txt_files(sbd_val, voc_val, merged_val)

# 4. テスト: 画像とラベルの対応確認
print("Testing image-label correspondence...")

def check_correspondence(split_file, jpg_dir, seg_dir):
    with open(split_file, 'r') as f:
        files = f.read().splitlines()
    
    mismatched = []
    for file in tqdm(files):
        jpg_path = os.path.join(jpg_dir, f"{file}.jpg")
        png_path = os.path.join(seg_dir, f"{file}.png")
        if not (os.path.exists(jpg_path) and os.path.exists(png_path)):
            mismatched.append(file)
    
    return mismatched

train_mismatched = check_correspondence(merged_train, merged_jpg_dir, merged_seg_dir)
val_mismatched = check_correspondence(merged_val, merged_jpg_dir, merged_seg_dir)

if not train_mismatched and not val_mismatched:
    print("Correlation is correct")
else:
    print("Mismatches found. Creating relation.txt...")
    relation_file = os.path.join(vocsbd_dir, "split", "relation.txt")
    with open(relation_file, 'w') as f:
        f.write("Image_Name\tJPEG_Exists\tPNG_Exists\n")
        all_files = set(os.listdir(merged_jpg_dir) + os.listdir(merged_seg_dir))
        for file in all_files:
            name = os.path.splitext(file)[0]
            jpg_exists = os.path.exists(os.path.join(merged_jpg_dir, f"{name}.jpg"))
            png_exists = os.path.exists(os.path.join(merged_seg_dir, f"{name}.png"))
            f.write(f"{name}\t{jpg_exists}\t{png_exists}\n")

print("Data integration and testing completed.")

================
File: make_dataset/download/download_sbd.py
================
import os
import shutil
from torchvision.datasets import VOCSegmentation
from tqdm import tqdm

def download_voc(root, year, image_set='train', download=True):
    try:
        dataset = VOCSegmentation(root=root, year=year, image_set=image_set, download=download)
        print(f"Successfully downloaded VOCSegmentation {year} {image_set}")
        return dataset
    except Exception as e:
        print(f"Failed to download VOCSegmentation {year} {image_set}: {str(e)}")
        return None

def download_sbd(root):
    sbd_dir = os.path.join(root, "SBD")
    if os.path.exists(sbd_dir) and os.path.isdir(sbd_dir):
        print("SBD dataset found. Skipping download.")
        return True
    else:
        print("SBD dataset not found. Please download it manually and place it in the correct directory.")
        return False

def copy_with_progress(src, dst, description):
    total_size = os.path.getsize(src)
    with tqdm(total=total_size, unit='B', unit_scale=True, desc=description) as pbar:
        with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:
            while True:
                buffer = fsrc.read(8192)
                if not buffer:
                    break
                fdst.write(buffer)
                pbar.update(len(buffer))

# Set the root directory for dataset download
root_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/VocSbd"

# Download PASCAL VOC 2012 (train and val)
voc_2012_dir = os.path.join(root_dir, "VOC2012")
voc_2012_train = download_voc(voc_2012_dir, "2012", "train")
voc_2012_val = download_voc(voc_2012_dir, "2012", "val")

# Download PASCAL VOC 2007 (test)
voc_2007_dir = os.path.join(root_dir, "VOC2007")
voc_2007_test = download_voc(voc_2007_dir, "2007", "test")

# Download SBD (or check if it exists)
sbd_dir = os.path.join(root_dir, "SBD")
sbd_dataset_exists = download_sbd(root_dir)

if voc_2012_train and voc_2012_val and voc_2007_test and sbd_dataset_exists:
    print("All datasets are available.")
    
    # Merge SBD into VOC2012 directory structure
    print("Merging SBD into VOC2012 directory structure...")
    
    # Copy SBD images
    sbd_img_dir = os.path.join(sbd_dir, "img")
    voc_img_dir = os.path.join(voc_2012_dir, "JPEGImages")
    os.makedirs(voc_img_dir, exist_ok=True)
    for img in tqdm(os.listdir(sbd_img_dir), desc="Copying SBD images"):
        src = os.path.join(sbd_img_dir, img)
        dst = os.path.join(voc_img_dir, img)
        if not os.path.exists(dst):
            copy_with_progress(src, dst, f"Copying {img}")

    # Copy SBD annotations
    sbd_cls_dir = os.path.join(sbd_dir, "cls")
    voc_seg_dir = os.path.join(voc_2012_dir, "SegmentationClass")
    os.makedirs(voc_seg_dir, exist_ok=True)
    for ann in tqdm(os.listdir(sbd_cls_dir), desc="Copying SBD annotations"):
        src = os.path.join(sbd_cls_dir, ann)
        dst = os.path.join(voc_seg_dir, ann)
        if not os.path.exists(dst):
            copy_with_progress(src, dst, f"Copying {ann}")

    print("Datasets merged successfully.")
else:
    print("Failed to download or locate one or more datasets.")

================
File: make_dataset/download/download_voc2007.sh
================
#!bin/bash

start=`date +%s`

cd ~

cd segmentaion/datasets/voc

echo "Downloading VOC2007 trainval ..."
curl -LO http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
echo "Downloading VOC2007 test data ..."
curl -LO http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
echo "Done downloading."

echo "Extracting trainval ..."
tar -xvf VOCtrainval_06-Nov-2007.tar
echo "Extracting test ..."
tar -xvf VOCtest_06-Nov-2007.tar
echo "Removing tars ..."
rm VOCtrainval_06-Nov-2007.tar
rm VOCtest_06-Nov-2007.tar

end=`date +%s`
runtime=$((end-start))

echo "Completed in" $runtime "seconds!!"

================
File: make_dataset/download/download_voc2012.sh
================
#!/bin/bash
# Script to download VOC2012 dataset

start=`date +%s`

cd ~

cd segmentaion/datasets/voc

# Download VOC2012
echo "Downloading VOC2012 trainval ..."
curl -LO http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
echo "Done downloading."

# Extract VOC2012 data
echo "Extracting VOC2012 trainval ..."
tar -xvf VOCtrainval_11-May-2012.tar

# Remove tar file
echo "Removing tar file ..."
rm VOCtrainval_11-May-2012.tar

end=`date +%s`
runtime=$((end-start))

echo "Completed in" $runtime "seconds"

================
File: make_dataset/download/merge.py
================
import os
import shutil
import scipy.io
import numpy as np
from PIL import Image
from tqdm import tqdm

def mat_to_png(mat_file, png_file):
    mat = scipy.io.loadmat(mat_file)
    segmentation = mat['GTcls']['Segmentation'][0][0]
    img = Image.fromarray(segmentation.astype(np.uint8))
    img.save(png_file)

root_dir = "/homes/ykohata/code/devml/homes/ypark/code/seg/dataset/VocSbd"
sbd_dir = os.path.join(root_dir, "SBD")
voc_dir = os.path.join(root_dir, "VOC2012")

# Create SegmentationClassAug directory
voc_aug_dir = os.path.join(voc_dir, "SegmentationClassAug")
os.makedirs(voc_aug_dir, exist_ok=True)

# Convert and copy SBD segmentation masks
print("Converting and copying SBD segmentation masks...")
sbd_cls_dir = os.path.join(sbd_dir, "cls")
for mat_file in tqdm(os.listdir(sbd_cls_dir)):
    if mat_file.endswith('.mat'):
        mat_path = os.path.join(sbd_cls_dir, mat_file)
        png_file = os.path.splitext(mat_file)[0] + '.png'
        png_path = os.path.join(voc_aug_dir, png_file)
        mat_to_png(mat_path, png_path)

# Copy train.txt and val.txt
print("Copying train.txt and val.txt...")
for file in ['train.txt', 'val.txt']:
    src = os.path.join(sbd_dir, file)
    dst = os.path.join(voc_dir, file)
    shutil.copy2(src, dst)

print("SBD dataset preparation completed.")
